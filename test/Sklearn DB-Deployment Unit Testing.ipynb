{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For now, need to be running sklearn==0.21.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import math\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'sklearn model deployment' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "from splicemachine.spark import PySpliceContext\n",
    "from splicemachine.mlflow_support.utilities import get_user\n",
    "from splicemachine.mlflow_support import *\n",
    "\n",
    "splice = PySpliceContext(spark)\n",
    "mlflow.set_experiment('sklearn model deployment')\n",
    "mlflow.register_splice_context(splice)\n",
    "schema = get_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file... Done.\n",
      "Deploying regression\n",
      "Deploying model to database...\n",
      "Your Job has been submitted. The returned value of this function is the job id, which you can use to monitor the your task in real-time. Run mlflow.watch_job(<job id>) tostream them to stdout, or mlflow.fetch_logs(<job id>) to read them one time to a list\n",
      "\n",
      "---Job Logs---\n",
      "INFO     2021-06-04 21:33:51.640 - A service worker has found your request\n",
      "INFO     2021-06-04 21:33:51.717 - Checking whether handler DEPLOY_DATABASE is enabled\n",
      "INFO     2021-06-04 21:33:51.797 - Handler is available\n",
      "INFO     2021-06-04 21:33:51.810 - Retrieving Run from MLFlow Tracking Server...\n",
      "INFO     2021-06-04 21:33:51.889 - Retrieved MLFlow Run\n",
      "INFO     2021-06-04 21:33:51.907 - Updating MLFlow Run for the UI\n",
      "INFO     2021-06-04 21:33:51.997 - Reading Model Artifact Stream from Splice Machine\n",
      "INFO     2021-06-04 21:33:52.015 - Extracting Model from DB with Name: regression_model\n",
      "INFO     2021-06-04 21:33:52.041 - Decoding Model Artifact Binary Stream for Deployment\n",
      "INFO     2021-06-04 21:33:52.105 - Decompressing Model Artifact\n",
      "INFO     2021-06-04 21:33:52.125 - Creating raw model representation from MLModel\n",
      "INFO     2021-06-04 21:33:52.144 - Reading MLModel Flavor from Extracted Archive\n",
      "INFO     2021-06-04 21:33:52.166 - Registering Raw Model Representation...\n",
      "INFO     2021-06-04 21:33:52.186 - Done.\n",
      "INFO     2021-06-04 21:33:52.204 - Adding Model Schema and DF...\n",
      "INFO     2021-06-04 21:33:52.247 - Creating Alternative Scikit Representations\n",
      "INFO     2021-06-04 21:33:52.267 - Registering Serialized Representation\n",
      "INFO     2021-06-04 21:33:52.300 - Updating Artifact with serialized representation\n",
      "INFO     2021-06-04 21:33:52.318 - Preparing Model Metadata for Deployment...\n",
      "INFO     2021-06-04 21:33:52.339 - Prepare Scikit-learn metadata for deployment\n",
      "INFO     2021-06-04 21:33:52.411 - Adding Schema String to model metadata...\n",
      "INFO     2021-06-04 21:33:52.431 - Starting 'creating model deployment table'...\n",
      "INFO     2021-06-04 21:33:52.477 - Executing\n",
      "CREATE TABLE SPLICE.SK_REGRESSION (\n",
      "                CUR_USER VARCHAR(50) DEFAULT CURRENT_USER,\n",
      "                EVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "                RUN_ID VARCHAR(50) DEFAULT '5bc88dbbdb58',\n",
      "                COL1 BIGINT, COL2 BIGINT,MOMENT_KEY INT,PREDICTION DOUBLE,PRIMARY KEY(MOMENT_KEY))\n",
      "INFO     2021-06-04 21:33:53.295 - Done with 'creating model deployment table' [in 864.2308712005615 ms]\n",
      "INFO     2021-06-04 21:33:53.315 - Starting 'creating trigger'...\n",
      "INFO     2021-06-04 21:33:53.336 - Creating VTI Prediction Trigger...\n",
      "INFO     2021-06-04 21:33:53.407 - Executing\n",
      "CREATE TRIGGER SPLICE.runModel_SK_REGRESSION_5bc88dbbdb58 \n",
      "                        AFTER INSERT ON SPLICE.SK_REGRESSION REFERENCING NEW TABLE AS NT FOR EACH STATEMENT\n",
      "                        UPDATE SPLICE.SK_REGRESSION --splice-properties useSpark=False \n",
      "SET (PREDICTION) = (SELECT b.PREDICTION FROM new \"com.splicemachine.mlrunner.MLRunner\"('regression', '5bc88dbbdb58',\n",
      "        new \"com.splicemachine.derby.catalog.TriggerNewTransitionRows\"(),'SPLICE', \n",
      "        'SK_REGRESSION', 'NULL', 'NULL', \n",
      "        cast(-1 as float), 'col1,col2', '', 10000)\n",
      "         as b (cur_user VARCHAR(50),eval_time TIMESTAMP,run_id VARCHAR(50),col1 BIGINT,col2 BIGINT,moment_key INTEGER,prediction FLOAT) --splice-properties useSpark=False, joinStrategy = sortmerge \n",
      "WHERE SPLICE.SK_REGRESSION.MOMENT_KEY = b.MOMENT_KEY)\n",
      "INFO     2021-06-04 21:33:53.683 - Done with 'creating trigger' [in 367.8600788116455 ms]\n",
      "INFO     2021-06-04 21:33:53.703 - Starting 'add model to metadata table'...\n",
      "INFO     2021-06-04 21:33:53.722 - Adding Model to Metadata table\n",
      "INFO     2021-06-04 21:33:53.806 - Executing SQL to insert Database Deployed Metadata\n",
      "INFO     2021-06-04 21:33:53.862 - Done executing.\n",
      "INFO     2021-06-04 21:33:53.881 - Done with 'add model to metadata table' [in 178.5125732421875 ms]\n",
      "INFO     2021-06-04 21:33:53.902 - Starting 'Managing Feature Store metadata'...\n",
      "INFO     2021-06-04 21:33:53.919 - Checking if run was created with Feature Store training set\n",
      "INFO     2021-06-04 21:33:53.952 - Done with 'Managing Feature Store metadata' [in 50.28247833251953 ms]\n",
      "INFO     2021-06-04 21:33:53.971 - Flushing\n",
      "INFO     2021-06-04 21:33:53.990 - Committing Transaction to Database\n",
      "INFO     2021-06-04 21:33:54.033 - Committed.\n",
      "INFO     2021-06-04 21:33:54.051 - Cleaning up deployment\n",
      "INFO     2021-06-04 21:33:54.147 - Success! Target 'DEPLOY_DATABASE' completed successfully.\n",
      "INFO     2021-06-04 21:33:54.173 - TASK_COMPLETEDUploading file... Done.\n",
      "Saved artifact as regression_run_log.ipynb in mlflow\n",
      "Uploading file... Done.\n",
      "Saved artifact as regression_run_log.html in mlflow\n",
      "Loading model 5bc88dbbdb58\n",
      "Downloading file regression_model\n",
      "Done. Unpacking artifact\n",
      "Running model\n",
      "Getting results\n",
      "Comparing DB results to model results\n",
      "test passed!\n"
     ]
    }
   ],
   "source": [
    "# Simple Regression\n",
    "\n",
    "import pandas as pd\n",
    "# Data\n",
    "d = {'col1': [1, 2], 'col2': [3, 4], 'y': [0.2, 5.3]}\n",
    "df = pd.DataFrame(data=d)\n",
    "df\n",
    "# Model\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(df[['col1', 'col2']], df['y'])\n",
    "# Deploy\n",
    "\n",
    "with mlflow.start_run(run_name='regression') as run:\n",
    "    mlflow.log_model(reg, 'regression_model')\n",
    "    splice.execute(f'drop table if exists {schema}.sk_regression')\n",
    "    print('Deploying regression')\n",
    "    jid = mlflow.deploy_db(schema, 'sk_regression', mlflow.current_run_id(), primary_key={'MOMENT_KEY': 'INT'}, df=df[['col1', 'col2']],create_model_table=True)\n",
    "    mlflow.watch_job(jid)\n",
    "    \n",
    "print(f'Loading model {run.info.run_uuid}')\n",
    "mlflow.load_model(run.info.run_uuid)\n",
    "print('Running model')\n",
    "splice.execute(\"insert into sk_regression (col1, col2, moment_key) values(1,5,4347861)\")    \n",
    "splice.execute(\"insert into sk_regression (col1, col2, moment_key) values(2,7,4908084)\")\n",
    "print('Getting results')\n",
    "data = splice.df('select col1, col2, prediction from sk_regression').toPandas()\n",
    "print('Comparing DB results to model results')\n",
    "for index, row in data.iterrows():\n",
    "    c1, c2, p = row\n",
    "    raw_p = reg.predict([[float(c1),float(c2)]])[0]\n",
    "    assert math.isclose(raw_p,p), f'Something is wrong. Model Table gives {p} but raw model gives {raw_p}'\n",
    "print('test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file... Done.\n",
      "Deploying sk_bayesian\n",
      "Deploying model to database...\n",
      "Your Job has been submitted. The returned value of this function is the job id, which you can use to monitor the your task in real-time. Run mlflow.watch_job(<job id>) tostream them to stdout, or mlflow.fetch_logs(<job id>) to read them one time to a list\n",
      "\n",
      "---Job Logs---\n",
      "INFO     2021-06-04 22:08:38.693 - A service worker has found your request\n",
      "INFO     2021-06-04 22:08:38.773 - Checking whether handler DEPLOY_DATABASE is enabled\n",
      "INFO     2021-06-04 22:08:38.811 - Handler is available\n",
      "INFO     2021-06-04 22:08:38.825 - Retrieving Run from MLFlow Tracking Server...\n",
      "INFO     2021-06-04 22:08:38.904 - Retrieved MLFlow Run\n",
      "INFO     2021-06-04 22:08:38.923 - Updating MLFlow Run for the UI\n",
      "INFO     2021-06-04 22:08:39.014 - Reading Model Artifact Stream from Splice Machine\n",
      "INFO     2021-06-04 22:08:39.033 - Extracting Model from DB with Name: bayesian_model\n",
      "INFO     2021-06-04 22:08:39.058 - Decoding Model Artifact Binary Stream for Deployment\n",
      "INFO     2021-06-04 22:08:39.123 - Decompressing Model Artifact\n",
      "INFO     2021-06-04 22:08:39.143 - Creating raw model representation from MLModel\n",
      "INFO     2021-06-04 22:08:39.163 - Reading MLModel Flavor from Extracted Archive\n",
      "INFO     2021-06-04 22:08:39.184 - Registering Raw Model Representation...\n",
      "INFO     2021-06-04 22:08:39.203 - Done.\n",
      "INFO     2021-06-04 22:08:39.221 - Adding Model Schema and DF...\n",
      "INFO     2021-06-04 22:08:39.267 - Creating Alternative Scikit Representations\n",
      "INFO     2021-06-04 22:08:39.287 - Registering Serialized Representation\n",
      "INFO     2021-06-04 22:08:39.325 - Updating Artifact with serialized representation\n",
      "INFO     2021-06-04 22:08:39.344 - Preparing Model Metadata for Deployment...\n",
      "INFO     2021-06-04 22:08:39.363 - Prepare Scikit-learn metadata for deployment\n",
      "INFO     2021-06-04 22:08:39.382 - Found predict arguments... classes are ['PREDICTION', 'std']\n",
      "INFO     2021-06-04 22:08:39.402 - Using sanitized labels ['PREDICTION', 'std'] as labels for predictions\n",
      "INFO     2021-06-04 22:08:39.473 - Adding Schema String to model metadata...\n",
      "INFO     2021-06-04 22:08:39.492 - Starting 'creating model deployment table'...\n",
      "INFO     2021-06-04 22:08:39.532 - Executing\n",
      "CREATE TABLE SPLICE.SK_BAYESIAN (\n",
      "                CUR_USER VARCHAR(50) DEFAULT CURRENT_USER,\n",
      "                EVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "                RUN_ID VARCHAR(50) DEFAULT '96e2d3c51682',\n",
      "                COL1 BIGINT, COL2 BIGINT,MOMENT_KEY INT,PREDICTION VARCHAR(5000),STD DOUBLE,PRIMARY KEY(MOMENT_KEY))\n",
      "INFO     2021-06-04 22:08:40.357 - Done with 'creating model deployment table' [in 864.8130893707275 ms]\n",
      "INFO     2021-06-04 22:08:40.379 - Starting 'creating trigger'...\n",
      "INFO     2021-06-04 22:08:40.399 - Creating VTI Prediction Trigger...\n",
      "INFO     2021-06-04 22:08:40.418 - Managing Sklearn prediction args\n",
      "INFO     2021-06-04 22:08:40.483 - Executing\n",
      "CREATE TRIGGER SPLICE.runModel_SK_BAYESIAN_96e2d3c51682 \n",
      "                        AFTER INSERT ON SPLICE.SK_BAYESIAN REFERENCING NEW TABLE AS NT FOR EACH STATEMENT\n",
      "                        UPDATE SPLICE.SK_BAYESIAN --splice-properties useSpark=False \n",
      "SET (PREDICTION,std) = (SELECT b.PREDICTION,b.std FROM new \"com.splicemachine.mlrunner.MLRunner\"('key_value', '96e2d3c51682',\n",
      "        new \"com.splicemachine.derby.catalog.TriggerNewTransitionRows\"(),'SPLICE', \n",
      "        'SK_BAYESIAN', 'predict', 'return_std', \n",
      "        cast(-1 as float), 'col1,col2', 'STD', 10000)\n",
      "         as b (cur_user VARCHAR(50),eval_time TIMESTAMP,run_id VARCHAR(50),col1 BIGINT,col2 BIGINT,moment_key INTEGER,prediction VARCHAR(5000),std FLOAT) --splice-properties useSpark=False, joinStrategy = sortmerge \n",
      "WHERE SPLICE.SK_BAYESIAN.MOMENT_KEY = b.MOMENT_KEY)\n",
      "INFO     2021-06-04 22:08:40.761 - Done with 'creating trigger' [in 382.39359855651855 ms]\n",
      "INFO     2021-06-04 22:08:40.781 - Starting 'add model to metadata table'...\n",
      "INFO     2021-06-04 22:08:40.800 - Adding Model to Metadata table\n",
      "INFO     2021-06-04 22:08:40.896 - Executing SQL to insert Database Deployed Metadata\n",
      "INFO     2021-06-04 22:08:40.952 - Done executing.\n",
      "INFO     2021-06-04 22:08:40.971 - Done with 'add model to metadata table' [in 189.73851203918457 ms]\n",
      "INFO     2021-06-04 22:08:40.989 - Starting 'Managing Feature Store metadata'...\n",
      "INFO     2021-06-04 22:08:41.008 - Checking if run was created with Feature Store training set\n",
      "INFO     2021-06-04 22:08:41.043 - Done with 'Managing Feature Store metadata' [in 53.78556251525879 ms]\n",
      "INFO     2021-06-04 22:08:41.063 - Flushing\n",
      "INFO     2021-06-04 22:08:41.082 - Committing Transaction to Database\n",
      "INFO     2021-06-04 22:08:41.128 - Committed.\n",
      "INFO     2021-06-04 22:08:41.149 - Cleaning up deployment\n",
      "INFO     2021-06-04 22:08:41.203 - Success! Target 'DEPLOY_DATABASE' completed successfully.\n",
      "INFO     2021-06-04 22:08:41.230 - TASK_COMPLETEDUploading file... Done.\n",
      "Saved artifact as bayesian with std_run_log.ipynb in mlflow\n",
      "Uploading file... Done.\n",
      "Saved artifact as bayesian with std_run_log.html in mlflow\n",
      "Loading model 96e2d3c51682\n",
      "Downloading file bayesian_model\n",
      "Done. Unpacking artifact\n",
      "Running model\n",
      "Getting results\n",
      "Comparing DB results to model results\n",
      "test passed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "import pandas as pd\n",
    "# Data\n",
    "d = {'col1': [1, 2], 'col2': [3, 4], 'y': [0.2, 5.3]}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "# Model\n",
    "model = BayesianRidge(compute_score=True,normalize = True)\n",
    "model.fit(df[['col1', 'col2']], df['y'])\n",
    "x = model.predict([[1,2]], return_std=True)\n",
    "\n",
    "# Deploy\n",
    "with mlflow.start_run(run_name='bayesian with std') as run:\n",
    "    mlflow.log_model(model, 'bayesian_model')\n",
    "    splice.execute(f'drop table if exists {schema}.sk_bayesian')\n",
    "    print('Deploying sk_bayesian')\n",
    "    jid = mlflow.deploy_db(schema, 'sk_bayesian', mlflow.current_run_id(), primary_key={'MOMENT_KEY': 'INT'}, df=df[['col1', 'col2']], create_model_table=True, library_specific={'predict_call':'predict', 'predict_args':'return_std'})\n",
    "    mlflow.watch_job(jid)\n",
    "    \n",
    "print(f'Loading model {run.info.run_uuid}')\n",
    "mlflow.load_model(run.info.run_uuid)\n",
    "print('Running model')\n",
    "splice.execute(\"insert into sk_bayesian (col1, col2, moment_key) values(-4,-5,1)\")\n",
    "splice.execute(\"insert into sk_bayesian (col1, col2, moment_key) values(100,22,2)\")\n",
    "splice.execute(\"insert into sk_bayesian (col1, col2, moment_key) values(7,8,3)\")\n",
    "print('Getting results')\n",
    "data = splice.df('select col1, col2, prediction, std from sk_bayesian').toPandas()\n",
    "print('Comparing DB results to model results')\n",
    "for index, row in data.iterrows():\n",
    "    c1, c2, p, std = row\n",
    "    raw_p, raw_std = model.predict([[float(c1),float(c2)]], return_std=True)\n",
    "    assert math.isclose(raw_p[0],float(p)), f'Something is wrong. Model Table gives {p} but raw model gives {raw_p}'\n",
    "    assert math.isclose(raw_std,std), f'Something is wrong. Model Table gives {std} but raw model gives {raw_std}'\n",
    "print('test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Bayesian model into existing table\n",
    "###  First 2 rows should have <b>NO</b> values for prediction and std (we inserted those rows before model deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying bayesian to existing table\n",
      "Deploying model to database...\n",
      "Your Job has been submitted. The returned value of this function is the job id, which you can use to monitor the your task in real-time. Run mlflow.watch_job(<job id>) tostream them to stdout, or mlflow.fetch_logs(<job id>) to read them one time to a list\n",
      "\n",
      "---Job Logs---\n",
      "INFO     2021-06-04 22:08:45.311 - A service worker has found your request\n",
      "INFO     2021-06-04 22:08:45.388 - Checking whether handler DEPLOY_DATABASE is enabled\n",
      "INFO     2021-06-04 22:08:45.468 - Handler is available\n",
      "INFO     2021-06-04 22:08:45.482 - Retrieving Run from MLFlow Tracking Server...\n",
      "INFO     2021-06-04 22:08:45.558 - Retrieved MLFlow Run\n",
      "INFO     2021-06-04 22:08:45.578 - Updating MLFlow Run for the UI\n",
      "INFO     2021-06-04 22:08:45.745 - Reading Model Artifact Stream from Splice Machine\n",
      "INFO     2021-06-04 22:08:45.764 - Extracting Model from DB with Name: bayesian_model\n",
      "INFO     2021-06-04 22:08:45.788 - Decoding Model Artifact Binary Stream for Deployment\n",
      "INFO     2021-06-04 22:08:45.812 - Decompressing Model Artifact\n",
      "INFO     2021-06-04 22:08:45.831 - Creating raw model representation from MLModel\n",
      "INFO     2021-06-04 22:08:45.850 - Reading MLModel Flavor from Extracted Archive\n",
      "INFO     2021-06-04 22:08:45.873 - Registering Raw Model Representation...\n",
      "INFO     2021-06-04 22:08:45.891 - Done.\n",
      "INFO     2021-06-04 22:08:45.976 - Creating Alternative Scikit Representations\n",
      "INFO     2021-06-04 22:08:45.996 - Registering Serialized Representation\n",
      "INFO     2021-06-04 22:08:46.026 - Updating Artifact with serialized representation\n",
      "INFO     2021-06-04 22:08:46.045 - Preparing Model Metadata for Deployment...\n",
      "INFO     2021-06-04 22:08:46.062 - Prepare Scikit-learn metadata for deployment\n",
      "INFO     2021-06-04 22:08:46.081 - Found predict arguments... classes are ['PREDICTION', 'std']\n",
      "INFO     2021-06-04 22:08:46.099 - Using sanitized labels ['PREDICTION', 'std'] as labels for predictions\n",
      "INFO     2021-06-04 22:08:46.171 - Adding Schema String to model metadata...\n",
      "INFO     2021-06-04 22:08:46.190 - Starting 'altering existing table'...\n",
      "INFO     2021-06-04 22:08:46.208 - Altering existing model...\n",
      "INFO     2021-06-04 22:08:46.288 - Executing\n",
      "ALTER TABLE SPLICE.BAYESIAN_TABLE ADD COLUMN CUR_USER VARCHAR(50) DEFAULT CURRENT_USER)\n",
      "INFO     2021-06-04 22:08:46.441 - Executing\n",
      "ALTER TABLE SPLICE.BAYESIAN_TABLE ADD COLUMN EVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP)\n",
      "INFO     2021-06-04 22:08:46.613 - Executing\n",
      "ALTER TABLE SPLICE.BAYESIAN_TABLE ADD COLUMN RUN_ID VARCHAR(50) DEFAULT '96e2d3c51682')\n",
      "INFO     2021-06-04 22:08:46.791 - Executing\n",
      "ALTER TABLE SPLICE.BAYESIAN_TABLE ADD COLUMN PREDICTION VARCHAR(5000))\n",
      "INFO     2021-06-04 22:08:46.896 - Executing\n",
      "ALTER TABLE SPLICE.BAYESIAN_TABLE ADD COLUMN STD DOUBLE)\n",
      "INFO     2021-06-04 22:08:47.009 - Done with 'altering existing table' [in 818.4123039245605 ms]\n",
      "INFO     2021-06-04 22:08:47.027 - Starting 'creating trigger'...\n",
      "INFO     2021-06-04 22:08:47.046 - Creating VTI Prediction Trigger...\n",
      "INFO     2021-06-04 22:08:47.064 - Managing Sklearn prediction args\n",
      "INFO     2021-06-04 22:08:47.122 - Executing\n",
      "CREATE TRIGGER SPLICE.runModel_BAYESIAN_TABLE_96e2d3c51682 \n",
      "                        AFTER INSERT ON SPLICE.BAYESIAN_TABLE REFERENCING NEW TABLE AS NT FOR EACH STATEMENT\n",
      "                        UPDATE SPLICE.BAYESIAN_TABLE --splice-properties useSpark=False \n",
      "SET (PREDICTION,std) = (SELECT b.PREDICTION,b.std FROM new \"com.splicemachine.mlrunner.MLRunner\"('key_value', '96e2d3c51682',\n",
      "        new \"com.splicemachine.derby.catalog.TriggerNewTransitionRows\"(),'SPLICE', \n",
      "        'BAYESIAN_TABLE', 'predict', 'return_std', \n",
      "        cast(-1 as float), 'COL1,COL2', 'STD', 10000)\n",
      "         as b (col1 INTEGER,col2 INTEGER,moment_key INTEGER,cur_user VARCHAR(50),eval_time TIMESTAMP,run_id VARCHAR(50),prediction VARCHAR(5000),std FLOAT) --splice-properties useSpark=False, joinStrategy = sortmerge \n",
      "WHERE SPLICE.BAYESIAN_TABLE.MOMENT_KEY = b.MOMENT_KEY)\n",
      "INFO     2021-06-04 22:08:47.385 - Done with 'creating trigger' [in 357.6486110687256 ms]\n",
      "INFO     2021-06-04 22:08:47.405 - Starting 'add model to metadata table'...\n",
      "INFO     2021-06-04 22:08:47.424 - Adding Model to Metadata table\n",
      "INFO     2021-06-04 22:08:47.509 - Executing SQL to insert Database Deployed Metadata\n",
      "INFO     2021-06-04 22:08:47.571 - Done executing.\n",
      "INFO     2021-06-04 22:08:47.591 - Done with 'add model to metadata table' [in 186.11502647399902 ms]\n",
      "INFO     2021-06-04 22:08:47.609 - Starting 'Managing Feature Store metadata'...\n",
      "INFO     2021-06-04 22:08:47.627 - Checking if run was created with Feature Store training set\n",
      "INFO     2021-06-04 22:08:47.659 - Done with 'Managing Feature Store metadata' [in 49.832820892333984 ms]\n",
      "INFO     2021-06-04 22:08:47.678 - Flushing\n",
      "INFO     2021-06-04 22:08:47.697 - Committing Transaction to Database\n",
      "INFO     2021-06-04 22:08:47.771 - Committed.\n",
      "INFO     2021-06-04 22:08:47.789 - Cleaning up deployment\n",
      "INFO     2021-06-04 22:08:47.843 - Success! Target 'DEPLOY_DATABASE' completed successfully.\n",
      "INFO     2021-06-04 22:08:47.867 - TASK_COMPLETEDtest passed!\n"
     ]
    }
   ],
   "source": [
    "setup_sql = \"\"\"\n",
    "drop table if exists bayesian_table;\n",
    "create table bayesian_table(col1 int, col2 int, moment_key int primary key);\n",
    "insert into bayesian_table values(4,2,1);\n",
    "insert into bayesian_table values(9,8,7);\n",
    "select * from bayesian_table\n",
    "\"\"\".split(';')\n",
    "for sql in setup_sql:\n",
    "    splice.execute(sql.strip())\n",
    "\n",
    "print('Deploying bayesian to existing table')\n",
    "jid = mlflow.deploy_db(schema, 'bayesian_table', run.info.run_uuid, model_cols=['COL1','COL2'], library_specific={'predict_call':'predict', 'predict_args':'return_std'}, create_model_table=False)\n",
    "mlflow.watch_job(jid)\n",
    "\n",
    "print('Running model')\n",
    "splice.execute('insert into bayesian_table (col1, col2, moment_key) values(4,5,20)')\n",
    "splice.execute('insert into bayesian_table (col1, col2, moment_key) values(-4,-5,21)')\n",
    "splice.execute('insert into bayesian_table (col1, col2, moment_key) values(7,8,22)')\n",
    "\n",
    "print('Getting results')\n",
    "data = splice.df('select col1, col2, prediction, std from bayesian_table').toPandas()\n",
    "\n",
    "print('Comparing DB results to model results')\n",
    "for index, row in data.iterrows():\n",
    "    c1, c2, p, std = row\n",
    "    if index in (0, 1):\n",
    "        assert not p or math.isnan(p), f\"Something is wrong. prediction should be NaN for the first 2 rows but has value {p}\"\n",
    "        assert not std or math.isnan(std), f\"Something is wrong. std should be NaN for the first 2 rows but has value {std}\"\n",
    "    else:\n",
    "        raw_p, raw_std = model.predict([[float(c1),float(c2)]], return_std=True)\n",
    "        assert math.isclose(raw_p[0], float(p)), f'Something is wrong. Model Table gives {p} but raw model gives {raw_p}'\n",
    "        assert math.isclose(raw_std[0], float(std)), f'Something is wrong. Model Table gives {std} but raw model gives {raw_std}'\n",
    "print('test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.49311640498059234'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM Model\n",
    "## GMM with Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file... Done.\n",
      "Deploying model to database...\n",
      "Your Job has been submitted. The returned value of this function is the job id, which you can use to monitor the your task in real-time. Run mlflow.watch_job(<job id>) tostream them to stdout, or mlflow.fetch_logs(<job id>) to read them one time to a list\n",
      "\n",
      "---Job Logs---\n",
      "INFO     2021-06-04 22:04:40.061 - A service worker has found your request\n",
      "INFO     2021-06-04 22:04:40.139 - Checking whether handler DEPLOY_DATABASE is enabled\n",
      "INFO     2021-06-04 22:04:40.219 - Handler is available\n",
      "INFO     2021-06-04 22:04:40.235 - Retrieving Run from MLFlow Tracking Server...\n",
      "INFO     2021-06-04 22:04:40.321 - Retrieved MLFlow Run\n",
      "INFO     2021-06-04 22:04:40.342 - Updating MLFlow Run for the UI\n",
      "INFO     2021-06-04 22:04:40.434 - Reading Model Artifact Stream from Splice Machine\n",
      "INFO     2021-06-04 22:04:40.452 - Extracting Model from DB with Name: gmm_model\n",
      "INFO     2021-06-04 22:04:40.482 - Decoding Model Artifact Binary Stream for Deployment\n",
      "INFO     2021-06-04 22:04:40.505 - Decompressing Model Artifact\n",
      "INFO     2021-06-04 22:04:40.525 - Creating raw model representation from MLModel\n",
      "INFO     2021-06-04 22:04:40.544 - Reading MLModel Flavor from Extracted Archive\n",
      "INFO     2021-06-04 22:04:40.568 - Registering Raw Model Representation...\n",
      "INFO     2021-06-04 22:04:40.585 - Done.\n",
      "INFO     2021-06-04 22:04:40.605 - Adding Model Schema and DF...\n",
      "INFO     2021-06-04 22:04:40.644 - Creating Alternative Scikit Representations\n",
      "INFO     2021-06-04 22:04:40.664 - Registering Serialized Representation\n",
      "INFO     2021-06-04 22:04:40.710 - Updating Artifact with serialized representation\n",
      "INFO     2021-06-04 22:04:40.729 - Preparing Model Metadata for Deployment...\n",
      "INFO     2021-06-04 22:04:40.746 - Prepare Scikit-learn metadata for deployment\n",
      "INFO     2021-06-04 22:04:40.766 - Found predict arguments... classes are ['PREDICTION', 'std']\n",
      "INFO     2021-06-04 22:04:40.783 - Using sanitized labels ['PREDICTION', 'std'] as labels for predictions\n",
      "INFO     2021-06-04 22:04:40.854 - Adding Schema String to model metadata...\n",
      "INFO     2021-06-04 22:04:40.873 - Starting 'creating model deployment table'...\n",
      "INFO     2021-06-04 22:04:40.919 - Executing\n",
      "CREATE TABLE SPLICE.SK_GMM (\n",
      "                CUR_USER VARCHAR(50) DEFAULT CURRENT_USER,\n",
      "                EVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "                RUN_ID VARCHAR(50) DEFAULT '029cd5543458',\n",
      "                COL1 BIGINT, COL2 BIGINT,MOMENT_KEY INT,PREDICTION VARCHAR(5000),STD DOUBLE,PRIMARY KEY(MOMENT_KEY))\n",
      "INFO     2021-06-04 22:04:41.739 - Done with 'creating model deployment table' [in 866.0497665405273 ms]\n",
      "INFO     2021-06-04 22:04:41.759 - Starting 'creating trigger'...\n",
      "INFO     2021-06-04 22:04:41.779 - Creating VTI Prediction Trigger...\n",
      "INFO     2021-06-04 22:04:41.799 - Managing Sklearn prediction args\n",
      "INFO     2021-06-04 22:04:41.871 - Executing\n",
      "CREATE TRIGGER SPLICE.runModel_SK_GMM_029cd5543458 \n",
      "                        AFTER INSERT ON SPLICE.SK_GMM REFERENCING NEW TABLE AS NT FOR EACH STATEMENT\n",
      "                        UPDATE SPLICE.SK_GMM --splice-properties useSpark=False \n",
      "SET (PREDICTION,std) = (SELECT b.PREDICTION,b.std FROM new \"com.splicemachine.mlrunner.MLRunner\"('key_value', '029cd5543458',\n",
      "        new \"com.splicemachine.derby.catalog.TriggerNewTransitionRows\"(),'SPLICE', \n",
      "        'SK_GMM', 'predict', 'return_std', \n",
      "        cast(-1 as float), 'col1,col2', 'STD', 10000)\n",
      "         as b (cur_user VARCHAR(50),eval_time TIMESTAMP,run_id VARCHAR(50),col1 BIGINT,col2 BIGINT,moment_key INTEGER,prediction VARCHAR(5000),std FLOAT) --splice-properties useSpark=False, joinStrategy = sortmerge \n",
      "WHERE SPLICE.SK_GMM.MOMENT_KEY = b.MOMENT_KEY)\n",
      "INFO     2021-06-04 22:04:42.198 - Done with 'creating trigger' [in 439.0449523925781 ms]\n",
      "INFO     2021-06-04 22:04:42.217 - Starting 'add model to metadata table'...\n",
      "INFO     2021-06-04 22:04:42.237 - Adding Model to Metadata table\n",
      "INFO     2021-06-04 22:04:42.422 - Executing SQL to insert Database Deployed Metadata\n",
      "INFO     2021-06-04 22:04:42.480 - Done executing.\n",
      "INFO     2021-06-04 22:04:42.499 - Done with 'add model to metadata table' [in 282.1850776672363 ms]\n",
      "INFO     2021-06-04 22:04:42.519 - Starting 'Managing Feature Store metadata'...\n",
      "INFO     2021-06-04 22:04:42.538 - Checking if run was created with Feature Store training set\n",
      "INFO     2021-06-04 22:04:42.578 - Done with 'Managing Feature Store metadata' [in 58.35914611816406 ms]\n",
      "INFO     2021-06-04 22:04:42.595 - Flushing\n",
      "INFO     2021-06-04 22:04:42.614 - Committing Transaction to Database\n",
      "INFO     2021-06-04 22:04:42.659 - Committed.\n",
      "INFO     2021-06-04 22:04:42.678 - Cleaning up deployment\n",
      "INFO     2021-06-04 22:04:42.773 - Success! Target 'DEPLOY_DATABASE' completed successfully.\n",
      "INFO     2021-06-04 22:04:42.801 - TASK_COMPLETEDUploading file... Done.\n",
      "Saved artifact as gmm with std_run_log.ipynb in mlflow\n",
      "Uploading file... Done.\n",
      "Saved artifact as gmm with std_run_log.html in mlflow\n",
      "Loading model 029cd5543458\n",
      "Downloading file gmm_model\n",
      "Done. Unpacking artifact\n",
      "Running model\n",
      "Getting results\n",
      "Comparing DB results to model results\n",
      "test passed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.gpr import GaussianProcessRegressor\n",
    "# Data\n",
    "d = {'col1': [1, 2], 'col2': [3, 4], 'y': [0.2, 5.3]}\n",
    "df = pd.DataFrame(data=d)\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "\n",
    "# Model\n",
    "g = GaussianProcessRegressor()\n",
    "g.fit(df[['col1','col2']],df['y'])\n",
    "# print(g.predict([[5,5]],return_std=True))\n",
    "# print(g.predict([[5,5]],return_cov=True))\n",
    "\n",
    "# Deploy\n",
    "with mlflow.start_run(run_name='gmm with std') as run:\n",
    "    mlflow.log_model(g, 'gmm_model')\n",
    "    splice.execute(f'drop table if exists {schema}.sk_gmm')\n",
    "    jid = mlflow.deploy_db(schema, 'sk_gmm', mlflow.current_run_id(), df=df[['col1', 'col2']], primary_key={'MOMENT_KEY': 'INT'}, create_model_table=True, library_specific={'predict_call':'predict', 'predict_args':'return_std'})\n",
    "    mlflow.watch_job(jid)\n",
    "    \n",
    "print(f'Loading model {run.info.run_uuid}')\n",
    "mlflow.load_model(run.info.run_uuid)\n",
    "\n",
    "print('Running model')\n",
    "splice.execute('insert into sk_gmm (col1,col2,moment_key) values(4,5,1)')\n",
    "splice.execute('insert into sk_gmm (col1,col2,moment_key) values(-4,-5,2)')\n",
    "splice.execute('insert into sk_gmm (col1,col2,moment_key) values(7,8,3)')\n",
    "\n",
    "print('Getting results')\n",
    "data = splice.df('select col1, col2, prediction, std from sk_gmm').toPandas()\n",
    "\n",
    "print('Comparing DB results to model results')\n",
    "for index, row in data.iterrows():\n",
    "    c1, c2, p, std = row\n",
    "    raw_p, raw_std = g.predict([[float(c1),float(c2)]], return_std=True)\n",
    "    assert math.isclose(raw_p[0], float(p)), f'Something is wrong. Model Table gives {p} but raw model gives {raw_p}'\n",
    "    assert math.isclose(raw_std[0], float(std)), f'Something is wrong. Model Table gives {std} but raw model gives {raw_std}'\n",
    "print('test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM with Covarience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file... Done.\n",
      "Deploying model to database...\n",
      "Your Job has been submitted. The returned value of this function is the job id, which you can use to monitor the your task in real-time. Run mlflow.watch_job(<job id>) tostream them to stdout, or mlflow.fetch_logs(<job id>) to read them one time to a list\n",
      "\n",
      "---Job Logs---\n",
      "INFO     2021-06-04 22:07:07.879 - A service worker has found your request\n",
      "INFO     2021-06-04 22:07:07.966 - Checking whether handler DEPLOY_DATABASE is enabled\n",
      "INFO     2021-06-04 22:07:08.046 - Handler is available\n",
      "INFO     2021-06-04 22:07:08.060 - Retrieving Run from MLFlow Tracking Server...\n",
      "INFO     2021-06-04 22:07:08.139 - Retrieved MLFlow Run\n",
      "INFO     2021-06-04 22:07:08.159 - Updating MLFlow Run for the UI\n",
      "INFO     2021-06-04 22:07:08.250 - Reading Model Artifact Stream from Splice Machine\n",
      "INFO     2021-06-04 22:07:08.267 - Extracting Model from DB with Name: gmm\n",
      "INFO     2021-06-04 22:07:08.295 - Decoding Model Artifact Binary Stream for Deployment\n",
      "INFO     2021-06-04 22:07:08.318 - Decompressing Model Artifact\n",
      "INFO     2021-06-04 22:07:08.338 - Creating raw model representation from MLModel\n",
      "INFO     2021-06-04 22:07:08.355 - Reading MLModel Flavor from Extracted Archive\n",
      "INFO     2021-06-04 22:07:08.379 - Registering Raw Model Representation...\n",
      "INFO     2021-06-04 22:07:08.397 - Done.\n",
      "INFO     2021-06-04 22:07:08.416 - Adding Model Schema and DF...\n",
      "INFO     2021-06-04 22:07:08.457 - Creating Alternative Scikit Representations\n",
      "INFO     2021-06-04 22:07:08.477 - Registering Serialized Representation\n",
      "INFO     2021-06-04 22:07:08.512 - Updating Artifact with serialized representation\n",
      "INFO     2021-06-04 22:07:08.531 - Preparing Model Metadata for Deployment...\n",
      "INFO     2021-06-04 22:07:08.549 - Prepare Scikit-learn metadata for deployment\n",
      "INFO     2021-06-04 22:07:08.569 - Found predict arguments... classes are ['PREDICTION', 'cov']\n",
      "INFO     2021-06-04 22:07:08.586 - Using sanitized labels ['PREDICTION', 'cov'] as labels for predictions\n",
      "INFO     2021-06-04 22:07:08.657 - Adding Schema String to model metadata...\n",
      "INFO     2021-06-04 22:07:08.675 - Starting 'creating model deployment table'...\n",
      "INFO     2021-06-04 22:07:08.721 - Executing\n",
      "CREATE TABLE SPLICE.SK_GMM_COV (\n",
      "                CUR_USER VARCHAR(50) DEFAULT CURRENT_USER,\n",
      "                EVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "                RUN_ID VARCHAR(50) DEFAULT '616d02e315b7',\n",
      "                COL1 BIGINT, COL2 BIGINT,MOMENT_KEY INT,PREDICTION VARCHAR(5000),COV DOUBLE,PRIMARY KEY(MOMENT_KEY))\n",
      "INFO     2021-06-04 22:07:09.545 - Done with 'creating model deployment table' [in 869.9824810028076 ms]\n",
      "INFO     2021-06-04 22:07:09.565 - Starting 'creating trigger'...\n",
      "INFO     2021-06-04 22:07:09.583 - Creating VTI Prediction Trigger...\n",
      "INFO     2021-06-04 22:07:09.602 - Managing Sklearn prediction args\n",
      "INFO     2021-06-04 22:07:09.672 - Executing\n",
      "CREATE TRIGGER SPLICE.runModel_SK_GMM_COV_616d02e315b7 \n",
      "                        AFTER INSERT ON SPLICE.SK_GMM_COV REFERENCING NEW TABLE AS NT FOR EACH STATEMENT\n",
      "                        UPDATE SPLICE.SK_GMM_COV --splice-properties useSpark=False \n",
      "SET (PREDICTION,cov) = (SELECT b.PREDICTION,b.cov FROM new \"com.splicemachine.mlrunner.MLRunner\"('key_value', '616d02e315b7',\n",
      "        new \"com.splicemachine.derby.catalog.TriggerNewTransitionRows\"(),'SPLICE', \n",
      "        'SK_GMM_COV', 'predict', 'return_cov', \n",
      "        cast(-1 as float), 'col1,col2', 'COV', 10000)\n",
      "         as b (cur_user VARCHAR(50),eval_time TIMESTAMP,run_id VARCHAR(50),col1 BIGINT,col2 BIGINT,moment_key INTEGER,prediction VARCHAR(5000),cov FLOAT) --splice-properties useSpark=False, joinStrategy = sortmerge \n",
      "WHERE SPLICE.SK_GMM_COV.MOMENT_KEY = b.MOMENT_KEY)\n",
      "INFO     2021-06-04 22:07:09.989 - Done with 'creating trigger' [in 424.47447776794434 ms]\n",
      "INFO     2021-06-04 22:07:10.008 - Starting 'add model to metadata table'...\n",
      "INFO     2021-06-04 22:07:10.029 - Adding Model to Metadata table\n",
      "INFO     2021-06-04 22:07:10.213 - Executing SQL to insert Database Deployed Metadata\n",
      "INFO     2021-06-04 22:07:10.273 - Done executing.\n",
      "INFO     2021-06-04 22:07:10.291 - Done with 'add model to metadata table' [in 282.09447860717773 ms]\n",
      "INFO     2021-06-04 22:07:10.309 - Starting 'Managing Feature Store metadata'...\n",
      "INFO     2021-06-04 22:07:10.327 - Checking if run was created with Feature Store training set\n",
      "INFO     2021-06-04 22:07:10.364 - Done with 'Managing Feature Store metadata' [in 54.723501205444336 ms]\n",
      "INFO     2021-06-04 22:07:10.383 - Flushing\n",
      "INFO     2021-06-04 22:07:10.404 - Committing Transaction to Database\n",
      "INFO     2021-06-04 22:07:10.450 - Committed.\n",
      "INFO     2021-06-04 22:07:10.470 - Cleaning up deployment\n",
      "INFO     2021-06-04 22:07:10.525 - Success! Target 'DEPLOY_DATABASE' completed successfully.\n",
      "INFO     2021-06-04 22:07:10.554 - TASK_COMPLETEDUploading file... Done.\n",
      "Saved artifact as gmm with cov_run_log.ipynb in mlflow\n",
      "Uploading file... Done.\n",
      "Saved artifact as gmm with cov_run_log.html in mlflow\n",
      "Loading model 616d02e315b7\n",
      "Downloading file gmm\n",
      "Done. Unpacking artifact\n",
      "Running model\n",
      "Getting results\n",
      "Comparing DB results to model results\n",
      "test passed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.gpr import GaussianProcessRegressor\n",
    "# Data\n",
    "d = {'col1': [1, 2], 'col2': [3, 4], 'y': [0.2, 5.3]}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "# Model\n",
    "g = GaussianProcessRegressor()\n",
    "g.fit(df[['col1','col2']],df['y'])\n",
    "# print(g.predict([[5,5]],return_std=True))\n",
    "# print(g.predict([[5,5]],return_cov=True))\n",
    "\n",
    "# Deploy\n",
    "with mlflow.start_run(run_name='gmm with cov') as run:\n",
    "    mlflow.log_model(g, 'gmm')\n",
    "    splice.execute(f'drop table if exists {schema}.sk_gmm_cov')\n",
    "    jid = mlflow.deploy_db(schema, 'sk_gmm_cov', mlflow.current_run_id(), df=df[['col1', 'col2']], primary_key={'MOMENT_KEY': 'INT'}, create_model_table=True, library_specific={'predict_call':'predict', 'predict_args':'return_cov'})\n",
    "    mlflow.watch_job(jid)\n",
    "    \n",
    "print(f'Loading model {run.info.run_uuid}')\n",
    "mlflow.load_model(run.info.run_uuid)\n",
    "\n",
    "print('Running model')\n",
    "splice.execute('insert into sk_gmm_cov (col1,col2,moment_key) values(4,5,1)')\n",
    "splice.execute('insert into sk_gmm_cov (col1,col2,moment_key) values(-4,-5,2)')\n",
    "splice.execute('insert into sk_gmm_cov (col1,col2,moment_key) values(7,8,3)')\n",
    "\n",
    "print('Getting results')\n",
    "data = splice.df('select col1, col2, prediction, cov from sk_gmm_cov').toPandas()\n",
    "\n",
    "print('Comparing DB results to model results')\n",
    "for index, row in data.iterrows():\n",
    "    c1, c2, p, std = row\n",
    "    raw_p, raw_std = g.predict([[float(c1),float(c2)]], return_cov=True)\n",
    "    \n",
    "    assert math.isclose(float(raw_p[0]), float(p)), f'Something is wrong. Model Table gives {p} but raw model gives {raw_p[0]}'\n",
    "    assert  math.isclose(raw_std[0], float(std)), f'Something is wrong. Model Table gives {std} but raw model gives {raw_std}'\n",
    "print('test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file... Done.\n",
      "Deploying model to database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deprecated Parameter 'verbose'. Use mlflow.watch_job(<job id>) or mlflow.fetch_logs(<job id>) to get verbose output. Ignoring...\n",
      "Passing in primary keys as a list of tuples is deprecated. Use dictionary {column name: type}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Job has been submitted. The returned value of this function is the job id, which you can use to monitor the your task in real-time. Run mlflow.watch_job(<job id>) tostream them to stdout, or mlflow.fetch_logs(<job id>) to read them one time to a list\n",
      "\n",
      "---Job Logs---\n",
      "INFO     2021-06-04 22:17:02.917 - A service worker has found your request\n",
      "INFO     2021-06-04 22:17:03.009 - Checking whether handler DEPLOY_DATABASE is enabled\n",
      "INFO     2021-06-04 22:17:03.062 - Handler is available\n",
      "INFO     2021-06-04 22:17:03.077 - Retrieving Run from MLFlow Tracking Server...\n",
      "INFO     2021-06-04 22:17:03.167 - Retrieved MLFlow Run\n",
      "INFO     2021-06-04 22:17:03.188 - Updating MLFlow Run for the UI\n",
      "INFO     2021-06-04 22:17:03.286 - Reading Model Artifact Stream from Splice Machine\n",
      "INFO     2021-06-04 22:17:03.306 - Extracting Model from DB with Name: kpca\n",
      "INFO     2021-06-04 22:17:03.340 - Decoding Model Artifact Binary Stream for Deployment\n",
      "INFO     2021-06-04 22:17:03.370 - Decompressing Model Artifact\n",
      "INFO     2021-06-04 22:17:03.395 - Creating raw model representation from MLModel\n",
      "INFO     2021-06-04 22:17:03.421 - Reading MLModel Flavor from Extracted Archive\n",
      "INFO     2021-06-04 22:17:03.445 - Found module mlflow.sklearn\n",
      "INFO     2021-06-04 22:17:03.474 - Registering Raw Model Representation...\n",
      "INFO     2021-06-04 22:17:03.493 - Done.\n",
      "INFO     2021-06-04 22:17:03.512 - Adding Model Schema and DF...\n",
      "INFO     2021-06-04 22:17:03.557 - Creating Alternative Scikit Representations\n",
      "INFO     2021-06-04 22:17:03.576 - Registering Serialized Representation\n",
      "INFO     2021-06-04 22:17:03.635 - Updating Artifact with serialized representation\n",
      "INFO     2021-06-04 22:17:03.655 - Preparing Model Metadata for Deployment...\n",
      "INFO     2021-06-04 22:17:03.675 - Prepare Scikit-learn metadata for deployment\n",
      "INFO     2021-06-04 22:17:03.696 - Using transform operation with classes ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C27', 'C28', 'C29', 'C30', 'C31', 'C32', 'C33', 'C34', 'C35', 'C36', 'C37', 'C38', 'C39', 'C40', 'C41', 'C42', 'C43', 'C44', 'C45', 'C46', 'C47', 'C48', 'C49', 'C50', 'C51', 'C52', 'C53', 'C54', 'C55', 'C56', 'C57', 'C58', 'C59', 'C60', 'C61', 'C62', 'C63', 'C64', 'C65', 'C66', 'C67', 'C68', 'C69', 'C70', 'C71', 'C72', 'C73', 'C74', 'C75', 'C76', 'C77', 'C78', 'C79', 'C80', 'C81', 'C82', 'C83', 'C84', 'C85', 'C86', 'C87', 'C88', 'C89', 'C90', 'C91', 'C92', 'C93', 'C94', 'C95', 'C96', 'C97', 'C98', 'C99']\n",
      "INFO     2021-06-04 22:17:03.715 - Using sanitized labels ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C27', 'C28', 'C29', 'C30', 'C31', 'C32', 'C33', 'C34', 'C35', 'C36', 'C37', 'C38', 'C39', 'C40', 'C41', 'C42', 'C43', 'C44', 'C45', 'C46', 'C47', 'C48', 'C49', 'C50', 'C51', 'C52', 'C53', 'C54', 'C55', 'C56', 'C57', 'C58', 'C59', 'C60', 'C61', 'C62', 'C63', 'C64', 'C65', 'C66', 'C67', 'C68', 'C69', 'C70', 'C71', 'C72', 'C73', 'C74', 'C75', 'C76', 'C77', 'C78', 'C79', 'C80', 'C81', 'C82', 'C83', 'C84', 'C85', 'C86', 'C87', 'C88', 'C89', 'C90', 'C91', 'C92', 'C93', 'C94', 'C95', 'C96', 'C97', 'C98', 'C99'] as labels for predictions\n",
      "INFO     2021-06-04 22:17:03.788 - Adding Schema String to model metadata...\n",
      "INFO     2021-06-04 22:17:03.807 - Starting 'creating model deployment table'...\n",
      "INFO     2021-06-04 22:17:03.852 - Executing\n",
      "CREATE TABLE SPLICE.PCA (\n",
      "                CUR_USER VARCHAR(50) DEFAULT CURRENT_USER,\n",
      "                EVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "                RUN_ID VARCHAR(50) DEFAULT '0a19732b577f',\n",
      "                COL1 FLOAT, COL2 FLOAT, COL3 FLOAT,MOMENT_KEY INT,C0 DOUBLE,C1 DOUBLE,C2 DOUBLE,C3 DOUBLE,C4 DOUBLE,C5 DOUBLE,C6 DOUBLE,C7 DOUBLE,C8 DOUBLE,C9 DOUBLE,C10 DOUBLE,C11 DOUBLE,C12 DOUBLE,C13 DOUBLE,C14 DOUBLE,C15 DOUBLE,C16 DOUBLE,C17 DOUBLE,C18 DOUBLE,C19 DOUBLE,C20 DOUBLE,C21 DOUBLE,C22 DOUBLE,C23 DOUBLE,C24 DOUBLE,C25 DOUBLE,C26 DOUBLE,C27 DOUBLE,C28 DOUBLE,C29 DOUBLE,C30 DOUBLE,C31 DOUBLE,C32 DOUBLE,C33 DOUBLE,C34 DOUBLE,C35 DOUBLE,C36 DOUBLE,C37 DOUBLE,C38 DOUBLE,C39 DOUBLE,C40 DOUBLE,C41 DOUBLE,C42 DOUBLE,C43 DOUBLE,C44 DOUBLE,C45 DOUBLE,C46 DOUBLE,C47 DOUBLE,C48 DOUBLE,C49 DOUBLE,C50 DOUBLE,C51 DOUBLE,C52 DOUBLE,C53 DOUBLE,C54 DOUBLE,C55 DOUBLE,C56 DOUBLE,C57 DOUBLE,C58 DOUBLE,C59 DOUBLE,C60 DOUBLE,C61 DOUBLE,C62 DOUBLE,C63 DOUBLE,C64 DOUBLE,C65 DOUBLE,C66 DOUBLE,C67 DOUBLE,C68 DOUBLE,C69 DOUBLE,C70 DOUBLE,C71 DOUBLE,C72 DOUBLE,C73 DOUBLE,C74 DOUBLE,C75 DOUBLE,C76 DOUBLE,C77 DOUBLE,C78 DOUBLE,C79 DOUBLE,C80 DOUBLE,C81 DOUBLE,C82 DOUBLE,C83 DOUBLE,C84 DOUBLE,C85 DOUBLE,C86 DOUBLE,C87 DOUBLE,C88 DOUBLE,C89 DOUBLE,C90 DOUBLE,C91 DOUBLE,C92 DOUBLE,C93 DOUBLE,C94 DOUBLE,C95 DOUBLE,C96 DOUBLE,C97 DOUBLE,C98 DOUBLE,C99 DOUBLE,PRIMARY KEY(MOMENT_KEY))\n",
      "INFO     2021-06-04 22:17:04.575 - Done with 'creating model deployment table' [in 767.6186561584473 ms]\n",
      "INFO     2021-06-04 22:17:04.595 - Starting 'creating trigger'...\n",
      "INFO     2021-06-04 22:17:04.615 - Creating VTI Prediction Trigger...\n",
      "INFO     2021-06-04 22:17:04.635 - Managing Sklearn prediction args\n",
      "INFO     2021-06-04 22:17:04.766 - Executing\n",
      "CREATE TRIGGER SPLICE.runModel_PCA_0a19732b577f \n",
      "                        AFTER INSERT ON SPLICE.PCA REFERENCING NEW TABLE AS NT FOR EACH STATEMENT\n",
      "                        UPDATE SPLICE.PCA --splice-properties useSpark=False \n",
      "SET (C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11,C12,C13,C14,C15,C16,C17,C18,C19,C20,C21,C22,C23,C24,C25,C26,C27,C28,C29,C30,C31,C32,C33,C34,C35,C36,C37,C38,C39,C40,C41,C42,C43,C44,C45,C46,C47,C48,C49,C50,C51,C52,C53,C54,C55,C56,C57,C58,C59,C60,C61,C62,C63,C64,C65,C66,C67,C68,C69,C70,C71,C72,C73,C74,C75,C76,C77,C78,C79,C80,C81,C82,C83,C84,C85,C86,C87,C88,C89,C90,C91,C92,C93,C94,C95,C96,C97,C98,C99) = (SELECT b.C0,b.C1,b.C2,b.C3,b.C4,b.C5,b.C6,b.C7,b.C8,b.C9,b.C10,b.C11,b.C12,b.C13,b.C14,b.C15,b.C16,b.C17,b.C18,b.C19,b.C20,b.C21,b.C22,b.C23,b.C24,b.C25,b.C26,b.C27,b.C28,b.C29,b.C30,b.C31,b.C32,b.C33,b.C34,b.C35,b.C36,b.C37,b.C38,b.C39,b.C40,b.C41,b.C42,b.C43,b.C44,b.C45,b.C46,b.C47,b.C48,b.C49,b.C50,b.C51,b.C52,b.C53,b.C54,b.C55,b.C56,b.C57,b.C58,b.C59,b.C60,b.C61,b.C62,b.C63,b.C64,b.C65,b.C66,b.C67,b.C68,b.C69,b.C70,b.C71,b.C72,b.C73,b.C74,b.C75,b.C76,b.C77,b.C78,b.C79,b.C80,b.C81,b.C82,b.C83,b.C84,b.C85,b.C86,b.C87,b.C88,b.C89,b.C90,b.C91,b.C92,b.C93,b.C94,b.C95,b.C96,b.C97,b.C98,b.C99 FROM new \"com.splicemachine.mlrunner.MLRunner\"('key_value', '0a19732b577f',\n",
      "        new \"com.splicemachine.derby.catalog.TriggerNewTransitionRows\"(),'SPLICE', \n",
      "        'PCA', 'transform', 'NULL', \n",
      "        cast(-1 as float), 'col1,col2,col3', 'C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11,C12,C13,C14,C15,C16,C17,C18,C19,C20,C21,C22,C23,C24,C25,C26,C27,C28,C29,C30,C31,C32,C33,C34,C35,C36,C37,C38,C39,C40,C41,C42,C43,C44,C45,C46,C47,C48,C49,C50,C51,C52,C53,C54,C55,C56,C57,C58,C59,C60,C61,C62,C63,C64,C65,C66,C67,C68,C69,C70,C71,C72,C73,C74,C75,C76,C77,C78,C79,C80,C81,C82,C83,C84,C85,C86,C87,C88,C89,C90,C91,C92,C93,C94,C95,C96,C97,C98,C99', 10000)\n",
      "         as b (cur_user VARCHAR(50),eval_time TIMESTAMP,run_id VARCHAR(50),col1 FLOAT,col2 FLOAT,col3 FLOAT,moment_key INTEGER,c0 FLOAT,c1 FLOAT,c2 FLOAT,c3 FLOAT,c4 FLOAT,c5 FLOAT,c6 FLOAT,c7 FLOAT,c8 FLOAT,c9 FLOAT,c10 FLOAT,c11 FLOAT,c12 FLOAT,c13 FLOAT,c14 FLOAT,c15 FLOAT,c16 FLOAT,c17 FLOAT,c18 FLOAT,c19 FLOAT,c20 FLOAT,c21 FLOAT,c22 FLOAT,c23 FLOAT,c24 FLOAT,c25 FLOAT,c26 FLOAT,c27 FLOAT,c28 FLOAT,c29 FLOAT,c30 FLOAT,c31 FLOAT,c32 FLOAT,c33 FLOAT,c34 FLOAT,c35 FLOAT,c36 FLOAT,c37 FLOAT,c38 FLOAT,c39 FLOAT,c40 FLOAT,c41 FLOAT,c42 FLOAT,c43 FLOAT,c44 FLOAT,c45 FLOAT,c46 FLOAT,c47 FLOAT,c48 FLOAT,c49 FLOAT,c50 FLOAT,c51 FLOAT,c52 FLOAT,c53 FLOAT,c54 FLOAT,c55 FLOAT,c56 FLOAT,c57 FLOAT,c58 FLOAT,c59 FLOAT,c60 FLOAT,c61 FLOAT,c62 FLOAT,c63 FLOAT,c64 FLOAT,c65 FLOAT,c66 FLOAT,c67 FLOAT,c68 FLOAT,c69 FLOAT,c70 FLOAT,c71 FLOAT,c72 FLOAT,c73 FLOAT,c74 FLOAT,c75 FLOAT,c76 FLOAT,c77 FLOAT,c78 FLOAT,c79 FLOAT,c80 FLOAT,c81 FLOAT,c82 FLOAT,c83 FLOAT,c84 FLOAT,c85 FLOAT,c86 FLOAT,c87 FLOAT,c88 FLOAT,c89 FLOAT,c90 FLOAT,c91 FLOAT,c92 FLOAT,c93 FLOAT,c94 FLOAT,c95 FLOAT,c96 FLOAT,c97 FLOAT,c98 FLOAT,c99 FLOAT) --splice-properties useSpark=False, joinStrategy = sortmerge \n",
      "WHERE SPLICE.PCA.MOMENT_KEY = b.MOMENT_KEY)\n",
      "INFO     2021-06-04 22:17:05.738 - Done with 'creating trigger' [in 1142.6560878753662 ms]\n",
      "INFO     2021-06-04 22:17:05.758 - Starting 'add model to metadata table'...\n",
      "INFO     2021-06-04 22:17:05.777 - Adding Model to Metadata table\n",
      "INFO     2021-06-04 22:17:05.967 - Executing SQL to insert Database Deployed Metadata\n",
      "INFO     2021-06-04 22:17:06.027 - Done executing.\n",
      "INFO     2021-06-04 22:17:06.048 - Done with 'add model to metadata table' [in 289.34764862060547 ms]\n",
      "INFO     2021-06-04 22:17:06.068 - Starting 'Managing Feature Store metadata'...\n",
      "INFO     2021-06-04 22:17:06.088 - Checking if run was created with Feature Store training set\n",
      "INFO     2021-06-04 22:17:06.123 - Done with 'Managing Feature Store metadata' [in 54.68869209289551 ms]\n",
      "INFO     2021-06-04 22:17:06.144 - Flushing\n",
      "INFO     2021-06-04 22:17:06.162 - Committing Transaction to Database\n",
      "INFO     2021-06-04 22:17:06.222 - Committed.\n",
      "INFO     2021-06-04 22:17:06.243 - Cleaning up deployment\n",
      "INFO     2021-06-04 22:17:06.339 - Success! Target 'DEPLOY_DATABASE' completed successfully.\n",
      "INFO     2021-06-04 22:17:06.369 - TASK_COMPLETEDUploading file... Done.\n",
      "Saved artifact as pca_run_log.ipynb in mlflow\n",
      "Uploading file... Done.\n",
      "Saved artifact as pca_run_log.html in mlflow\n",
      "Loading model 0a19732b577f\n",
      "Downloading file kpca\n",
      "Done. Unpacking artifact\n",
      "Running model\n",
      "Getting results\n",
      "Comparing DB results to model results\n",
      "test passed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from random import random\n",
    "\n",
    "\n",
    "kpca = KernelPCA(kernel=\"rbf\", gamma=10, n_components=100)\n",
    "d = []\n",
    "for i in range(500):\n",
    "    d.append([random()*i for _ in range(3)])\n",
    "    \n",
    "df = pd.DataFrame(data=d, columns=['col1', 'col2', 'col3'])\n",
    "\n",
    "kpca.get_params()\n",
    "kpca.fit(df[['col1','col2', 'col3']])\n",
    "kpca.transform([[1,2,3]])\n",
    "\n",
    "# Deploy\n",
    "with mlflow.start_run(run_name='pca') as run:\n",
    "    mlflow.log_model(kpca, 'kpca')\n",
    "    splice.execute(f'drop table if exists {schema}.pca')\n",
    "    jid = mlflow.deploy_db(schema, 'pca', mlflow.current_run_id(), create_model_table=True, df=df[['col1', 'col2', 'col3']], primary_key={'MOMENT_KEY': 'INT'}, library_specific={'predict_call':'transform'})\n",
    "    mlflow.watch_job(jid)\n",
    "\n",
    "    \n",
    "print(f'Loading model {run.info.run_uuid}')\n",
    "mlflow.load_model(run.info.run_uuid)\n",
    "\n",
    "print('Running model')\n",
    "splice.execute('insert into pca (col1,col2,col3,moment_key) values(1,-2,3,4)')\n",
    "splice.execute('insert into pca (col1,col2,col3,moment_key) values(-2,-3,33,5)')\n",
    "splice.execute('insert into pca (col1,col2,col3,moment_key) values(3,4,-23,6)')\n",
    "splice.execute('insert into pca (col1,col2,col3,moment_key) values(66,234,-2,1)')\n",
    "\n",
    "print('Getting results')\n",
    "data = splice.df('''select col1, col2, col3, C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11,C12,C13,C14,C15,C16,C17,C18,C19,\n",
    "                    C20,C21,C22,C23,C24,C25,C26,C27,C28,C29,C30,C31,C32,C33,C34,C35,C36,C37,C38,C39,C40,C41,C42,C43,\n",
    "                    C44,C45,C46,C47,C48,C49,C50,C51,C52,C53,C54,C55,C56,C57,C58,C59,C60,C61,C62,C63,C64,C65,C66,C67,\n",
    "                    C68,C69,C70,C71,C72,C73,C74,C75,C76,C77,C78,C79,C80,C81,C82,C83,C84,C85,C86,C87,C88,C89,C90,C91,\n",
    "                    C92,C93,C94,C95,C96,C97,C98,C99 from pca''').toPandas()\n",
    "\n",
    "print('Comparing DB results to model results')\n",
    "for index, row in data.iterrows():\n",
    "    pcs = row[[f'C{i}' for i in range(100)]].values\n",
    "    c1, c2, c3 = row['COL1'],row['COL2'],row['COL3']\n",
    "    raw_pcs = kpca.transform([[float(c1),float(c2), float(c3)]])[0]\n",
    "    if not (raw_pcs == pcs).all(): \n",
    "        print(f'Something is wrong. Checking rounding errors')\n",
    "        comp=0\n",
    "        for i,j in zip(raw_pcs, pcs):\n",
    "            for rnd in range(15,8,-1):\n",
    "                if round(i,rnd)!=round(j,rnd):\n",
    "                    raise Exception(f'Values are incorrect. Database returned {j} but model returned {i} for component C{comp}')\n",
    "                break\n",
    "        print(f'All values match to at least {rnd} decimal places')\n",
    "    \n",
    "print('test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file... Done.\n",
      "Deploying model to database...\n",
      "Your Job has been submitted. The returned value of this function is the job id, which you can use to monitor the your task in real-time. Run mlflow.watch_job(<job id>) tostream them to stdout, or mlflow.fetch_logs(<job id>) to read them one time to a list\n",
      "\n",
      "---Job Logs---\n",
      "INFO     2021-06-04 22:20:37.214 - A service worker has found your request\n",
      "INFO     2021-06-04 22:20:37.269 - Checking whether handler DEPLOY_DATABASE is enabled\n",
      "INFO     2021-06-04 22:20:37.314 - Handler is available\n",
      "INFO     2021-06-04 22:20:37.329 - Retrieving Run from MLFlow Tracking Server...\n",
      "INFO     2021-06-04 22:20:37.418 - Retrieved MLFlow Run\n",
      "INFO     2021-06-04 22:20:37.437 - Updating MLFlow Run for the UI\n",
      "INFO     2021-06-04 22:20:37.498 - Reading Model Artifact Stream from Splice Machine\n",
      "INFO     2021-06-04 22:20:37.517 - Extracting Model from DB with Name: pipeline\n",
      "INFO     2021-06-04 22:20:37.548 - Decoding Model Artifact Binary Stream for Deployment\n",
      "INFO     2021-06-04 22:20:37.574 - Decompressing Model Artifact\n",
      "INFO     2021-06-04 22:20:37.597 - Creating raw model representation from MLModel\n",
      "INFO     2021-06-04 22:20:37.617 - Reading MLModel Flavor from Extracted Archive\n",
      "INFO     2021-06-04 22:20:37.649 - Registering Raw Model Representation...\n",
      "INFO     2021-06-04 22:20:37.670 - Done.\n",
      "INFO     2021-06-04 22:20:37.690 - Adding Model Schema and DF...\n",
      "INFO     2021-06-04 22:20:37.735 - Creating Alternative Scikit Representations\n",
      "INFO     2021-06-04 22:20:37.758 - Registering Serialized Representation\n",
      "INFO     2021-06-04 22:20:37.800 - Updating Artifact with serialized representation\n",
      "INFO     2021-06-04 22:20:37.820 - Preparing Model Metadata for Deployment...\n",
      "INFO     2021-06-04 22:20:37.839 - Prepare Scikit-learn metadata for deployment\n",
      "INFO     2021-06-04 22:20:37.871 - Adding Schema String to model metadata...\n",
      "INFO     2021-06-04 22:20:37.892 - Starting 'creating model deployment table'...\n",
      "INFO     2021-06-04 22:20:37.940 - Executing\n",
      "CREATE TABLE SPLICE.SKPIPE (\n",
      "                CUR_USER VARCHAR(50) DEFAULT CURRENT_USER,\n",
      "                EVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "                RUN_ID VARCHAR(50) DEFAULT 'e68960245477',\n",
      "                C0 FLOAT, C1 FLOAT, C2 FLOAT, C3 FLOAT, C4 FLOAT, C5 FLOAT, C6 FLOAT, C7 FLOAT, C8 FLOAT, C9 FLOAT, C10 FLOAT, C11 FLOAT, C12 FLOAT, C13 FLOAT, C14 FLOAT, C15 FLOAT, C16 FLOAT, C17 FLOAT, C18 FLOAT, C19 FLOAT,MOMENT_KEY INT,PREDICTION INT,PRIMARY KEY(MOMENT_KEY))\n",
      "INFO     2021-06-04 22:20:38.799 - Done with 'creating model deployment table' [in 906.7821502685547 ms]\n",
      "INFO     2021-06-04 22:20:38.819 - Starting 'creating trigger'...\n",
      "INFO     2021-06-04 22:20:38.839 - Creating VTI Prediction Trigger...\n",
      "INFO     2021-06-04 22:20:38.915 - Executing\n",
      "CREATE TRIGGER SPLICE.runModel_SKPIPE_e68960245477 \n",
      "                        AFTER INSERT ON SPLICE.SKPIPE REFERENCING NEW TABLE AS NT FOR EACH STATEMENT\n",
      "                        UPDATE SPLICE.SKPIPE --splice-properties useSpark=False \n",
      "SET (PREDICTION) = (SELECT b.PREDICTION FROM new \"com.splicemachine.mlrunner.MLRunner\"('cluster', 'e68960245477',\n",
      "        new \"com.splicemachine.derby.catalog.TriggerNewTransitionRows\"(),'SPLICE', \n",
      "        'SKPIPE', 'NULL', 'NULL', \n",
      "        cast(-1 as float), 'C0,C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11,C12,C13,C14,C15,C16,C17,C18,C19', '', 10000)\n",
      "         as b (cur_user VARCHAR(50),eval_time TIMESTAMP,run_id VARCHAR(50),c0 FLOAT,c1 FLOAT,c2 FLOAT,c3 FLOAT,c4 FLOAT,c5 FLOAT,c6 FLOAT,c7 FLOAT,c8 FLOAT,c9 FLOAT,c10 FLOAT,c11 FLOAT,c12 FLOAT,c13 FLOAT,c14 FLOAT,c15 FLOAT,c16 FLOAT,c17 FLOAT,c18 FLOAT,c19 FLOAT,moment_key INTEGER,prediction INTEGER) --splice-properties useSpark=False, joinStrategy = sortmerge \n",
      "WHERE SPLICE.SKPIPE.MOMENT_KEY = b.MOMENT_KEY)\n",
      "INFO     2021-06-04 22:20:39.348 - Done with 'creating trigger' [in 528.4945964813232 ms]\n",
      "INFO     2021-06-04 22:20:39.368 - Starting 'add model to metadata table'...\n",
      "INFO     2021-06-04 22:20:39.388 - Adding Model to Metadata table\n",
      "INFO     2021-06-04 22:20:39.578 - Executing SQL to insert Database Deployed Metadata\n",
      "INFO     2021-06-04 22:20:39.638 - Done executing.\n",
      "INFO     2021-06-04 22:20:39.658 - Done with 'add model to metadata table' [in 289.7799015045166 ms]\n",
      "INFO     2021-06-04 22:20:39.677 - Starting 'Managing Feature Store metadata'...\n",
      "INFO     2021-06-04 22:20:39.695 - Checking if run was created with Feature Store training set\n",
      "INFO     2021-06-04 22:20:39.730 - Done with 'Managing Feature Store metadata' [in 53.073883056640625 ms]\n",
      "INFO     2021-06-04 22:20:39.749 - Flushing\n",
      "INFO     2021-06-04 22:20:39.769 - Committing Transaction to Database\n",
      "INFO     2021-06-04 22:20:39.815 - Committed.\n",
      "INFO     2021-06-04 22:20:39.835 - Cleaning up deployment\n",
      "INFO     2021-06-04 22:20:39.888 - Success! Target 'DEPLOY_DATABASE' completed successfully.\n",
      "INFO     2021-06-04 22:20:39.915 - TASK_COMPLETEDUploading file... Done.\n",
      "Saved artifact as pipeline_run_log.ipynb in mlflow\n",
      "Uploading file... Done.\n",
      "Saved artifact as pipeline_run_log.html in mlflow\n",
      "Loading model e68960245477\n",
      "Downloading file pipeline\n",
      "Done. Unpacking artifact\n",
      "Running model\n",
      "Getting results\n",
      "Comparing DB results to model results\n",
      "test passed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# generate some data to play with\n",
    "X, y = make_classification(\n",
    "    n_informative=5, n_redundant=0, random_state=42)\n",
    "# ANOVA SVM-C\n",
    "anova_filter = SelectKBest(f_regression, k=5)\n",
    "clf = svm.SVC(kernel='linear', probability=True)\n",
    "anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\n",
    "anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\n",
    "prediction = anova_svm.predict(X)\n",
    "anova_svm.score(X, y)\n",
    "# getting the selected features chosen by anova_filter\n",
    "anova_svm['anova'].get_support()\n",
    "# Another way to get selected features chosen by anova_filter\n",
    "anova_svm.named_steps.anova.get_support()\n",
    "# Indexing can also be used to extract a sub-pipeline.\n",
    "sub_pipeline = anova_svm[:1]\n",
    "sub_pipeline\n",
    "coef = anova_svm[-1].coef_\n",
    "anova_svm['svc'] is anova_svm[-1]\n",
    "coef.shape\n",
    "\n",
    "df = pd.DataFrame(X, columns=[f'C{i}' for i in range(20)])\n",
    "df['label'] = pd.DataFrame(y)\n",
    "\n",
    "# Deploy\n",
    "with mlflow.start_run(run_name='pipeline') as run:\n",
    "    mlflow.log_model(anova_svm, 'pipeline')\n",
    "    splice.execute(f'drop table if exists {schema}.skpipe')\n",
    "    jid = mlflow.deploy_db(schema, 'skpipe', mlflow.current_run_id(), primary_key={'MOMENT_KEY': 'INT'}, create_model_table=True, df=df.drop('label', axis=1))\n",
    "    mlflow.watch_job(jid)\n",
    "\n",
    "print(f'Loading model {run.info.run_uuid}')\n",
    "mlflow.load_model(run.info.run_uuid)\n",
    "\n",
    "print('Running model')\n",
    "splice.execute('insert into skpipe (c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,moment_key) values(1.1520228167464979,2.1605391309255357,2.1567734709026443,2.574915424636612,1.286824753247393,0.5889507491535976,1.5086880856146783,2.269695379745943,2.6089968335162177,0.14727247523071063,1.8485328196592175,2.097532276755521,0.27822480992530085,2.666733308369378,2.143893385597627,1.3465081988790233,0.11041583917168307,0.9246752875764862,2.500911791131701,1.1195218819285857,5)')\n",
    "splice.execute('insert into skpipe (c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,moment_key) values(3.611,6.133,9.065,9.347,4.649,11.534,1.436,3.625,0.617,5.556,1.191,11.871,6.27,13.615,4.942,16.17,1.858,13.121,9.192,16.729,9)')\n",
    "\n",
    "print('Getting results')\n",
    "data = splice.df('select c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19, prediction from skpipe').toPandas()\n",
    "\n",
    "print('Comparing DB results to model results')\n",
    "for index, row in data.iterrows():\n",
    "    c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,p = row\n",
    "    raw_p = anova_svm.predict([[float(c0),float(c1),float(c2),float(c3),float(c4),float(c5),float(c6),float(c7),float(c8),float(c9),float(c10),float(c11),float(c12),float(c13),float(c14),float(c15),float(c16),float(c17),float(c18),float(c19)]])\n",
    "    assert math.isclose(raw_p,p), f'Something is wrong. Model Table gives {p} but raw model gives {raw_p}'\n",
    "print('test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Proba Exmaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file... Done.\n",
      "Deploying model to database...\n",
      "Your Job has been submitted. The returned value of this function is the job id, which you can use to monitor the your task in real-time. Run mlflow.watch_job(<job id>) tostream them to stdout, or mlflow.fetch_logs(<job id>) to read them one time to a list\n",
      "\n",
      "---Job Logs---\n",
      "INFO     2021-06-04 22:22:28.734 - A service worker has found your request\n",
      "INFO     2021-06-04 22:22:28.813 - Checking whether handler DEPLOY_DATABASE is enabled\n",
      "INFO     2021-06-04 22:22:28.853 - Handler is available\n",
      "INFO     2021-06-04 22:22:28.866 - Retrieving Run from MLFlow Tracking Server...\n",
      "INFO     2021-06-04 22:22:28.963 - Retrieved MLFlow Run\n",
      "INFO     2021-06-04 22:22:28.981 - Updating MLFlow Run for the UI\n",
      "INFO     2021-06-04 22:22:29.070 - Reading Model Artifact Stream from Splice Machine\n",
      "INFO     2021-06-04 22:22:29.088 - Extracting Model from DB with Name: rf\n",
      "INFO     2021-06-04 22:22:29.117 - Decoding Model Artifact Binary Stream for Deployment\n",
      "INFO     2021-06-04 22:22:29.141 - Decompressing Model Artifact\n",
      "INFO     2021-06-04 22:22:29.161 - Creating raw model representation from MLModel\n",
      "INFO     2021-06-04 22:22:29.180 - Reading MLModel Flavor from Extracted Archive\n",
      "INFO     2021-06-04 22:22:29.202 - Registering Raw Model Representation...\n",
      "INFO     2021-06-04 22:22:29.221 - Done.\n",
      "INFO     2021-06-04 22:22:29.241 - Adding Model Schema and DF...\n",
      "INFO     2021-06-04 22:22:29.281 - Creating Alternative Scikit Representations\n",
      "INFO     2021-06-04 22:22:29.300 - Registering Serialized Representation\n",
      "INFO     2021-06-04 22:22:29.343 - Updating Artifact with serialized representation\n",
      "INFO     2021-06-04 22:22:29.361 - Preparing Model Metadata for Deployment...\n",
      "INFO     2021-06-04 22:22:29.380 - Prepare Scikit-learn metadata for deployment\n",
      "INFO     2021-06-04 22:22:29.399 - Using model provided classes: ['C1', 'C4']\n",
      "INFO     2021-06-04 22:22:29.418 - Using sanitized labels ['PREDICTION', 'C1', 'C4'] as labels for predictions\n",
      "INFO     2021-06-04 22:22:29.489 - Adding Schema String to model metadata...\n",
      "INFO     2021-06-04 22:22:29.508 - Starting 'creating model deployment table'...\n",
      "INFO     2021-06-04 22:22:29.550 - Executing\n",
      "CREATE TABLE SPLICE.PREDICT_PROBA (\n",
      "                CUR_USER VARCHAR(50) DEFAULT CURRENT_USER,\n",
      "                EVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "                RUN_ID VARCHAR(50) DEFAULT '2174813b7756',\n",
      "                COL1 BIGINT, COL2 BIGINT,MOMENT_KEY INT,PREDICTION VARCHAR(5000),C1 DOUBLE,C4 DOUBLE,PRIMARY KEY(MOMENT_KEY))\n",
      "INFO     2021-06-04 22:22:30.376 - Done with 'creating model deployment table' [in 868.1271076202393 ms]\n",
      "INFO     2021-06-04 22:22:30.396 - Starting 'creating trigger'...\n",
      "INFO     2021-06-04 22:22:30.415 - Creating VTI Prediction Trigger...\n",
      "INFO     2021-06-04 22:22:30.434 - Managing Sklearn prediction args\n",
      "INFO     2021-06-04 22:22:30.495 - Executing\n",
      "CREATE TRIGGER SPLICE.runModel_PREDICT_PROBA_2174813b7756 \n",
      "                        AFTER INSERT ON SPLICE.PREDICT_PROBA REFERENCING NEW TABLE AS NT FOR EACH STATEMENT\n",
      "                        UPDATE SPLICE.PREDICT_PROBA --splice-properties useSpark=False \n",
      "SET (PREDICTION,C1,C4) = (SELECT b.PREDICTION,b.C1,b.C4 FROM new \"com.splicemachine.mlrunner.MLRunner\"('key_value', '2174813b7756',\n",
      "        new \"com.splicemachine.derby.catalog.TriggerNewTransitionRows\"(),'SPLICE', \n",
      "        'PREDICT_PROBA', 'predict_proba', 'NULL', \n",
      "        cast(-1 as float), 'col1,col2', 'C1,C4', 10000)\n",
      "         as b (cur_user VARCHAR(50),eval_time TIMESTAMP,run_id VARCHAR(50),col1 BIGINT,col2 BIGINT,moment_key INTEGER,prediction VARCHAR(5000),c1 FLOAT,c4 FLOAT) --splice-properties useSpark=False, joinStrategy = sortmerge \n",
      "WHERE SPLICE.PREDICT_PROBA.MOMENT_KEY = b.MOMENT_KEY)\n",
      "INFO     2021-06-04 22:22:30.763 - Done with 'creating trigger' [in 367.62499809265137 ms]\n",
      "INFO     2021-06-04 22:22:30.783 - Starting 'add model to metadata table'...\n",
      "INFO     2021-06-04 22:22:30.801 - Adding Model to Metadata table\n",
      "INFO     2021-06-04 22:22:30.976 - Executing SQL to insert Database Deployed Metadata\n",
      "INFO     2021-06-04 22:22:31.042 - Done executing.\n",
      "INFO     2021-06-04 22:22:31.061 - Done with 'add model to metadata table' [in 278.033971786499 ms]\n",
      "INFO     2021-06-04 22:22:31.080 - Starting 'Managing Feature Store metadata'...\n",
      "INFO     2021-06-04 22:22:31.099 - Checking if run was created with Feature Store training set\n",
      "INFO     2021-06-04 22:22:31.137 - Done with 'Managing Feature Store metadata' [in 57.596683502197266 ms]\n",
      "INFO     2021-06-04 22:22:31.158 - Flushing\n",
      "INFO     2021-06-04 22:22:31.176 - Committing Transaction to Database\n",
      "INFO     2021-06-04 22:22:31.223 - Committed.\n",
      "INFO     2021-06-04 22:22:31.243 - Cleaning up deployment\n",
      "INFO     2021-06-04 22:22:31.298 - Success! Target 'DEPLOY_DATABASE' completed successfully.\n",
      "INFO     2021-06-04 22:22:31.324 - TASK_COMPLETEDUploading file... Done.\n",
      "Saved artifact as predict_proba_run_log.ipynb in mlflow\n",
      "Uploading file... Done.\n",
      "Saved artifact as predict_proba_run_log.html in mlflow\n",
      "Loading model 2174813b7756\n",
      "Downloading file rf\n",
      "Done. Unpacking artifact\n",
      "Running model\n",
      "Getting results\n",
      "Comparing DB results to model results\n",
      "test passed!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "d = {'col1': [1, 2], 'col2': [3, 4], 'y': [4, 1]}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf = clf.fit(df[['col1','col2']],df['y'])\n",
    "clf.predict_proba([[0,1]])\n",
    "\n",
    "# Deploy\n",
    "with mlflow.start_run(run_name='predict_proba') as run:\n",
    "    mlflow.log_model(clf, 'rf')\n",
    "    splice.execute(f'drop table if exists {schema}.predict_proba')\n",
    "    jid = mlflow.deploy_db(schema, 'predict_proba', mlflow.current_run_id(), create_model_table=True, df=df[['col1', 'col2']], primary_key={'MOMENT_KEY': 'INT'}, library_specific={'predict_call':'predict_proba'})\n",
    "    mlflow.watch_job(jid)\n",
    "    \n",
    "    \n",
    "print(f'Loading model {run.info.run_uuid}')\n",
    "mlflow.load_model(run.info.run_uuid)\n",
    "\n",
    "print('Running model')\n",
    "splice.execute('insert into predict_proba (col1,col2,moment_key) values(3.611,6.133,9)')\n",
    "splice.execute('insert into predict_proba (col1,col2,moment_key) values(1.1520228167464979,2.1605,5)')\n",
    "\n",
    "print('Getting results')\n",
    "data = splice.df('select col1,col2, prediction, C1, C4 from predict_proba').toPandas()\n",
    "\n",
    "print('Comparing DB results to model results')\n",
    "for index, row in data.iterrows():\n",
    "    col1,col2,p,c1,c4 = row\n",
    "    raw_c1,raw_c4 = clf.predict_proba([[float(col1),float(col2)]])[0]\n",
    "    raw_p = clf.predict([[float(col1),float(col2)]])[0]\n",
    "    assert f'C{raw_p}' == p, f'Something is wrong. Model Table gives {p} but raw model gives {raw_p}'\n",
    "    assert math.isclose(raw_c1,c1), f'Something is wrong. Model Table gives {c1} but raw model gives {raw_c1}'\n",
    "    assert math.isclose(raw_c4,c4), f'Something is wrong. Model Table gives {c4} but raw model gives {raw_c4}'\n",
    "print('test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
