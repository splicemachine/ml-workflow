{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.7/site-packages (6.2.1)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from pytest) (0.10.1)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest) (0.13.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest) (20.3.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from pytest) (1.9.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest) (20.4)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.7/site-packages (from pytest) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pytest) (1.7.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->pytest) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest) (3.1.0)\n",
      "Collecting pytest-notebook\n",
      "  Downloading pytest_notebook-0.6.1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pytest>=3.5.0 in /opt/conda/lib/python3.7/site-packages (from pytest-notebook) (6.2.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from pytest-notebook) (3.2.0)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.7/site-packages (from pytest-notebook) (6.1.7)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.1.4-py3-none-any.whl (26 kB)\n",
      "Collecting nbdime\n",
      "  Downloading nbdime-3.1.0-py2.py3-none-any.whl (5.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.1 MB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbformat in /opt/conda/lib/python3.7/site-packages (from pytest-notebook) (5.0.7)\n",
      "Requirement already satisfied: attrs<21,>=19 in /opt/conda/lib/python3.7/site-packages (from pytest-notebook) (20.3.0)\n",
      "Requirement already satisfied: nbconvert~=5.6.0 in /opt/conda/lib/python3.7/site-packages (from pytest-notebook) (5.6.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from pytest>=3.5.0->pytest-notebook) (1.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pytest>=3.5.0->pytest-notebook) (1.7.0)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from pytest>=3.5.0->pytest-notebook) (0.10.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest>=3.5.0->pytest-notebook) (20.4)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest>=3.5.0->pytest-notebook) (0.13.1)\n",
      "Requirement already satisfied: iniconfig in /opt/conda/lib/python3.7/site-packages (from pytest>=3.5.0->pytest-notebook) (1.0.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->pytest-notebook) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema->pytest-notebook) (49.6.0.post20200814)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->pytest-notebook) (0.17.3)\n",
      "Requirement already satisfied: tornado>=4.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-client->pytest-notebook) (5.1.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.7/site-packages (from jupyter-client->pytest-notebook) (4.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-client->pytest-notebook) (2.8.1)\n",
      "Requirement already satisfied: traitlets in /opt/conda/lib/python3.7/site-packages (from jupyter-client->pytest-notebook) (4.3.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.7/site-packages (from jupyter-client->pytest-notebook) (19.0.2)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /opt/conda/lib/python3.7/site-packages (from importlib-resources->pytest-notebook) (3.1.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from nbdime->pytest-notebook) (0.4.4)\n",
      "Collecting jupyter-server\n",
      "  Downloading jupyter_server-1.8.0-py3-none-any.whl (382 kB)\n",
      "\u001b[K     |████████████████████████████████| 382 kB 92.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: GitPython!=2.1.4,!=2.1.5,!=2.1.6 in /opt/conda/lib/python3.7/site-packages (from nbdime->pytest-notebook) (3.1.8)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from nbdime->pytest-notebook) (2.7.0)\n",
      "Collecting jupyter-server-mathjax>=0.2.2\n",
      "  Downloading jupyter_server_mathjax-0.2.3-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 86.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=2.9 in /opt/conda/lib/python3.7/site-packages (from nbdime->pytest-notebook) (2.11.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from nbdime->pytest-notebook) (2.24.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from nbformat->pytest-notebook) (0.2.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert~=5.6.0->pytest-notebook) (0.6.0)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert~=5.6.0->pytest-notebook) (3.1.5)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert~=5.6.0->pytest-notebook) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert~=5.6.0->pytest-notebook) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert~=5.6.0->pytest-notebook) (1.4.2)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert~=5.6.0->pytest-notebook) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest>=3.5.0->pytest-notebook) (2.4.7)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from traitlets->jupyter-client->pytest-notebook) (4.4.2)\n",
      "Collecting argon2-cffi\n",
      "  Downloading argon2_cffi-20.1.0-cp35-abi3-manylinux1_x86_64.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 12.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from jupyter-server->nbdime->pytest-notebook) (0.8.0)\n",
      "Collecting anyio<4,>=3.1.0\n",
      "  Downloading anyio-3.1.0-py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from jupyter-server->nbdime->pytest-notebook) (0.8.3)\n",
      "Requirement already satisfied: Send2Trash in /opt/conda/lib/python3.7/site-packages (from jupyter-server->nbdime->pytest-notebook) (1.5.0)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.7/site-packages (from jupyter-server->nbdime->pytest-notebook) (0.57.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython!=2.1.4,!=2.1.5,!=2.1.6->nbdime->pytest-notebook) (4.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2>=2.9->nbdime->pytest-notebook) (1.1.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->nbdime->pytest-notebook) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->nbdime->pytest-notebook) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->nbdime->pytest-notebook) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->nbdime->pytest-notebook) (2.10)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert~=5.6.0->pytest-notebook) (0.5.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->jupyter-server->nbdime->pytest-notebook) (1.14.1)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from anyio<4,>=3.1.0->jupyter-server->nbdime->pytest-notebook) (3.10.0.0)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython!=2.1.4,!=2.1.5,!=2.1.6->nbdime->pytest-notebook) (3.0.4)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->argon2-cffi->jupyter-server->nbdime->pytest-notebook) (2.20)\n",
      "Installing collected packages: importlib-resources, argon2-cffi, sniffio, anyio, jupyter-server, jupyter-server-mathjax, nbdime, pytest-notebook\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "jupyter-server 1.8.0 requires tornado>=6.1.0, but you'll have tornado 5.1.1 which is incompatible.\u001b[0m\n",
      "Successfully installed anyio-3.1.0 argon2-cffi-20.1.0 importlib-resources-5.1.4 jupyter-server-1.8.0 jupyter-server-mathjax-0.2.3 nbdime-3.1.0 pytest-notebook-0.6.1 sniffio-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest\n",
    "!pip install pytest-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-04 22:26:04--  https://raw.githubusercontent.com/akaihola/ipython_pytest/master/ipython_pytest.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 783 [text/plain]\n",
      "Saving to: ‘ipython_pytest.py’\n",
      "\n",
      "ipython_pytest.py   100%[===================>]     783  --.-KB/s    in 0s      \n",
      "\n",
      "2021-06-04 22:26:05 (20.7 MB/s) - ‘ipython_pytest.py’ saved [783/783]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/akaihola/ipython_pytest/master/ipython_pytest.py -O ipython_pytest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ipython_pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.7, pytest-6.2.1, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /tmp/tmpekaqqmnc\n",
      "plugins: notebook-0.6.1, anyio-3.1.0\n",
      "collected 9 items\n",
      "\n",
      "_ipytesttmp.py .........                                                 [100%]\n",
      "\n",
      "======================== 9 passed in 120.59s (0:02:00) =========================\n"
     ]
    }
   ],
   "source": [
    "%%pytest\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark import PySpliceContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from splicemachine.mlflow_support import *\n",
    "import pytest\n",
    "from splicemachine.mlflow_support import get_user\n",
    "\n",
    "schema = get_user()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "splice = PySpliceContext(spark)\n",
    "\n",
    "scoreSchema = [StructField(\"Score1\", DoubleType(), True),\n",
    "         StructField(\"Score2\", DoubleType(), True),\n",
    "         StructField(\"Result\", IntegerType(), True)]\n",
    "\n",
    "examSchema = StructType(scoreSchema)\n",
    "\n",
    "df = spark.read.schema(examSchema).load(\"s3a://splice-demo/spark_db_deployment_data.csv\", format=\"csv\").withColumnRenamed('Result','label')\n",
    "\n",
    "df.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"Score1\", \"Score2\"], outputCol=\"features\")\n",
    "\n",
    "testingSchema = [StructField(\"Score1\", DoubleType(), True),\n",
    "         StructField(\"Score2\", DoubleType(), True)]\n",
    "testValues = [(40.0,40.0),(20.0,80.0),(20.0,30.0),(90.0,80.0),(10.0,20.0),\n",
    "               (10.0,50.0),(50.0,30.0),(40.0,75.0),(90.0,30.0),(100.0,100.0)]\n",
    "testingDf = spark.createDataFrame(testValues, StructType(testingSchema))\n",
    "\n",
    "mlflow.register_splice_context(splice)\n",
    "\n",
    "\n",
    "def test_logistic_regression():\n",
    "    \n",
    "    print('========== Testing Logistic Regression ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='Logistic Regression'):\n",
    "    \n",
    "        model = LogisticRegression()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableLogisticRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableLogisticRegression\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        LogisticRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableLogisticRegression (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "\n",
    "        splice.execute(f\"select * from {schema}.ScoresTableLogisticRegression\")\n",
    "\n",
    "        for i,row in enumerate(LogisticRegressionDf.collect()):\n",
    "            dfp = [row[5],row[4][0],row[4][1]]\n",
    "            table_pred = list(splice.df(f'SELECT case when prediction=\\'C0\\' then 0 else 1 end PREDICTION,\"C0\",\"C1\" FROM SCORESTABLELOGISTICREGRESSION WHERE MOMENT_ID = {i}').collect()[0])\n",
    "            assert dfp == table_pred, f'Problem. {dfp} from model, {table_pred} from table'\n",
    "\n",
    "def test_decision_tree():\n",
    "    \n",
    "    print('========== Testing Decision Tree Classification ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='Decision Tree Classification'):\n",
    "        \n",
    "        model = DecisionTreeClassifier()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableDecisionTree\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableDecisionTree\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        DecisionTreeDf = model.transform(testingDf)\n",
    "\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableDecisionTree (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "\n",
    "        splice.execute(f\"select * from {schema}.ScoresTableDecisionTree\")\n",
    "\n",
    "        for i,row in enumerate(DecisionTreeDf.collect()):\n",
    "            dfp = [row[5],row[4][0],row[4][1]]\n",
    "            table_pred = list(splice.df(f'SELECT case when prediction=\\'C0\\' then 0 else 1 end PREDICTION,\"C0\",\"C1\" FROM {schema}.ScoresTableDecisionTree WHERE MOMENT_ID = {i}').collect()[0])\n",
    "            \n",
    "            for tab, d in zip(table_pred, dfp):\n",
    "                l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "                assert round(d,l) == round(tab,l), f'Problem. {d} from model, {tab} from table'\n",
    "        \n",
    "def test_random_forest():\n",
    "    \n",
    "    print('========== Testing Random Forest ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='Random Forest'):\n",
    "    \n",
    "        model = DecisionTreeClassifier()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableRandomForest\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableRandomForest\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)    \n",
    "            \n",
    "        RandomForestDf = model.transform(testingDf)\n",
    "\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableRandomForest (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        splice.execute(f\"select * from {schema}.ScoresTableRandomForest\")\n",
    "\n",
    "        for i,row in enumerate(RandomForestDf.collect()):\n",
    "            dfp = [row[5],row[4][0],row[4][1]]\n",
    "            table_pred = list(splice.df(f'SELECT case when prediction=\\'C0\\' then 0 else 1 end PREDICTION,\"C0\",\"C1\" FROM {schema}.ScoresTableRandomForest WHERE MOMENT_ID = {i}').collect()[0])\n",
    "            for tab, d in zip(table_pred, dfp):\n",
    "                l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "                assert round(d,l) == round(tab,l), f'Problem. {d} from model, {tab} from table'\n",
    "\n",
    "def test_gbt():  \n",
    "    \n",
    "    print('========== Testing GBT ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='GBT'):\n",
    "\n",
    "        model = GBTClassifier()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableGBT\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableGBT\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        GBTDf = model.transform(testingDf)\n",
    "\n",
    "        \n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableGBT (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        for i,row in enumerate(GBTDf.collect()):\n",
    "            dfp = [row[5],row[4][0],row[4][1]]\n",
    "            table_pred = list(splice.df(f'SELECT case when prediction=\\'C0\\' then 0 else 1 end PREDICTION,\"C0\",\"C1\" FROM {schema}.ScoresTableGBT WHERE MOMENT_ID = {i}').collect()[0])\n",
    "            for tab, d in zip(table_pred, dfp):\n",
    "                l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "                assert round(d,l) == round(tab,l), f'Problem. {d} from model, {tab} from table'\n",
    "\n",
    "def test_lsvc():\n",
    "    \n",
    "    print('========== Testing LSVC ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='Linear SVC'):\n",
    "        model = LinearSVC()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableLSVC\")\n",
    "\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableLSVC\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'})\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        LSVCDf = model.transform(testingDf)\n",
    "\n",
    "        LSVCDf.show()\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableLSVC (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        LSVCDf.show()\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            assert int(LSVCDf.collect()[i][-1]) == int(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLELSVC WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "\n",
    "def test_naive_bayes():\n",
    "    \n",
    "    print('========== Testing Naive Bayes ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='Naive Bayes'):\n",
    "        model = NaiveBayes()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "    \n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableNaiveBayes\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableNaiveBayes\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        NaiveBayesDf = model.transform(testingDf)\n",
    "\n",
    "        NaiveBayesDf.show()\n",
    "\n",
    "        \n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableNaiveBayes (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        NaiveBayesDf.show()\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            assert int(NaiveBayesDf.collect()[i][5]) == int(splice.df(f\"SELECT case when PREDICTION='C1' then 1 else 0 end PREDICTION FROM {schema}.SCORESTABLENAIVEBAYES WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "\n",
    "def test_decision_tree_regression():\n",
    "    \n",
    "    print('========== Testing Decision Tree Regression ==========')\n",
    "    with mlflow.start_run(run_name='Decision Tree Regression'):\n",
    "        model = DecisionTreeRegressor()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableDecisionTreeRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableDecisionTreeRegression\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'})\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        DecisionTreeRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        DecisionTreeRegressionDf.show()\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableDecisionTreeRegression (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        DecisionTreeRegressionDf.show()\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            assert float(DecisionTreeRegressionDf.collect()[i][3]) == float(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLEDECISIONTREEREGRESSION WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "\n",
    "def test_random_forest_regression():\n",
    "    \n",
    "    print('========== Testing Random Forest Regression ==========')\n",
    "    with mlflow.start_run(run_name='Random Forest Regression'):\n",
    "        model = RandomForestRegressor()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "    \n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableRandomForestRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableRandomForestRegression\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'})\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        RandomForestRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        RandomForestRegressionDf.show()\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableRandomForestRegression (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        RandomForestRegressionDf.show()\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            assert float(RandomForestRegressionDf.collect()[i][3]) == float(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLERANDOMFORESTREGRESSION WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "        \n",
    "def test_gradient_boosted_tree_regression():\n",
    "    \n",
    "    print('========== Testing GBT Regression ==========')\n",
    "    with mlflow.start_run(run_name='GBT Regression'):\n",
    "        model = GBTRegressor()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "    \n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableGBTRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableGBTRegression\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'})\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        GBTRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        GBTRegressionDf.show()\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableGBTRegression (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        GBTRegressionDf.show()\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            assert float(GBTRegressionDf.collect()[i][3]) == float(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLEGBTREGRESSION WHERE MOMENT_ID = {i}\").collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.7, pytest-6.2.1, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /tmp/tmp6fsbp46f\n",
      "plugins: notebook-0.6.1, anyio-3.1.0\n",
      "collected 4 items\n",
      "\n",
      "_ipytesttmp.py ....                                                      [100%]\n",
      "\n",
      "============================== 4 passed in 44.47s ==============================\n"
     ]
    }
   ],
   "source": [
    "%%pytest\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark import PySpliceContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from splicemachine.mlflow_support import *\n",
    "import pytest\n",
    "import pyspark.sql.functions as F\n",
    "from splicemachine.mlflow_support import get_user\n",
    "\n",
    "schema = get_user()\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "splice = PySpliceContext(spark)\n",
    "\n",
    "testValues = [(391,314,102,2,2.0,2.5,8.24,0,0.64),\n",
    "(392,318,106,3,2.0,3.0,8.65,0,0.71),\n",
    "(393,326,112,4,4.0,3.5,9.12,1,0.84),\n",
    "(394,317,104,2,3.0,3.0,8.76,0,0.77),\n",
    "(395,329,111,4,4.5,4.0,9.23,1,0.89),\n",
    "(396,324,110,3,3.5,3.5,9.04,1,0.82),\n",
    "(397,325,107,3,3.0,3.5,9.11,1,0.84),\n",
    "(398,330,116,4,5.0,4.5,9.45,1,0.91),\n",
    "(399,312,103,3,3.5,4.0,8.78,0,0.67),\n",
    "(400,333,117,4,5.0,4.0,9.66,1,0.95)]\n",
    "\n",
    "admitSchema = [StructField(\"Serial Number\", IntegerType(), True),\n",
    "        StructField(\"GRE\", IntegerType(), True),\n",
    "         StructField(\"TOEFL\", IntegerType(), True),\n",
    "        StructField(\"Rating\", IntegerType(), True),\n",
    "         StructField(\"SOP\", DoubleType(), True),\n",
    "        StructField(\"LOR\", DoubleType(), True),\n",
    "         StructField(\"CGPA\", DoubleType(), True),\n",
    "        StructField(\"Research\", IntegerType(), True),\n",
    "         StructField(\"label\", DoubleType(), True)]\n",
    "\n",
    "collegeSchema = StructType(admitSchema)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"GRE\", \"TOEFL\", \"Rating\", \"SOP\", \"LOR\", \"CGPA\", \"Research\"], outputCol=\"features\")\n",
    "\n",
    "testingDf = spark.createDataFrame(testValues, StructType(collegeSchema))\n",
    "\n",
    "mlflow.register_splice_context(splice)\n",
    "\n",
    "def test_linear_regression():\n",
    "    \n",
    "    print('========== Testing Linear Regression ==========')\n",
    "    with mlflow.start_run(run_name='Linear Regression'):\n",
    "        model = LinearRegression()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(testingDf)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableLinearRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableLinearRegression\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\").drop(\"Serial Number\"), create_model_table=True, primary_key={'MOMENT_ID': 'INT'})\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        LinearRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableLinearRegression (MOMENT_ID,GRE,TOEFL,Rating,SOP,LOR,CGPA,Research) VALUES (\" + str(moment_id) + \",\" + str(val[1]) + \",\" + str(val[2]) + \",\" + str(val[3]) + \",\" + str(val[4]) + \",\" + str(val[5]) + \",\" + str(val[6]) + \",\" + str(val[7]) + \")\")\n",
    "            \n",
    "        for i in range (len(testValues)):\n",
    "            d = float(LinearRegressionDf.collect()[i][10])\n",
    "            tab = float(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLELINEARREGRESSION WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "            l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "            assert round(d,l) == round(tab,l), f'Problem. {round(d,l)} from model, {round(tab,l)} from table'\n",
    "        \n",
    "def test_generalized_linear_regression():\n",
    "   \n",
    "    print('========== Testing Generalized Linear Regression ==========')\n",
    "    with mlflow.start_run(run_name='Generalized Linear Regression'):\n",
    "        model = GeneralizedLinearRegression()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(testingDf)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableGeneralizedLinearRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableGeneralizedLinearRegression\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\").drop(\"Serial Number\"), create_model_table=True, primary_key={'MOMENT_ID': 'INT'})\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        GeneralizedLinearRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableGeneralizedLinearRegression (MOMENT_ID,GRE,TOEFL,Rating,SOP,LOR,CGPA,Research) VALUES (\" + str(moment_id) + \",\" + str(val[1]) + \",\" + str(val[2]) + \",\" + str(val[3]) + \",\" + str(val[4]) + \",\" + str(val[5]) + \",\" + str(val[6]) + \",\" + str(val[7]) + \")\")\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            d = float(GeneralizedLinearRegressionDf.collect()[i][10])\n",
    "            tab = float(splice.df(f\"SELECT PREDICTION FROM {schema}.ScoresTableGeneralizedLinearRegression WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "            l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "            assert round(d,l) == round(tab,l), f'Problem. {round(d,l)} from model, {round(tab,l)} from table'\n",
    "\n",
    "\n",
    "def test_isotonic_regression():\n",
    "   \n",
    "    print('========== Testing Isotonic Regression ==========')\n",
    "    with mlflow.start_run(run_name='Isotonic Regression'):\n",
    "        model = IsotonicRegression()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(testingDf)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableIsotonicRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableIsotonicRegression\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\").drop(\"Serial Number\"), create_model_table=True, primary_key={'MOMENT_ID': 'INT'})\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        isotonicRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableIsotonicRegression (MOMENT_ID,GRE,TOEFL,Rating,SOP,LOR,CGPA,Research) VALUES (\" + str(moment_id) + \",\" + str(val[1]) + \",\" + str(val[2]) + \",\" + str(val[3]) + \",\" + str(val[4]) + \",\" + str(val[5]) + \",\" + str(val[6]) + \",\" + str(val[7]) + \")\")\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            d = float(isotonicRegressionDf.collect()[i][10])\n",
    "            tab = float(splice.df(f\"SELECT PREDICTION FROM {schema}.ScoresTableIsotonicRegression WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "            l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "            assert round(d,l) == round(tab,l), f'Problem. {round(d,l)} from model, {round(tab,l)} from table'\n",
    "            \n",
    "def test_survival_regression():\n",
    "   \n",
    "    print('========== Testing Survival Regression ==========')\n",
    "    \n",
    "    testValues = [\n",
    "        (1.218, 1.0),\n",
    "        (2.949, 0.0),\n",
    "        (3.627, 0.0),\n",
    "        (0.273, 1.0),\n",
    "        (4.199, 0.0)\n",
    "    ]\n",
    "    \n",
    "    testingDf = spark.createDataFrame(testValues, [\"label\", \"censor\"])\n",
    "    \n",
    "    with mlflow.start_run(run_name='Survival Regression'):\n",
    "        \n",
    "        assembler = VectorAssembler(inputCols=[\"censor\"], outputCol=\"features\")\n",
    "        quantileProbabilities = [0.3, 0.6]\n",
    "        model = AFTSurvivalRegression(quantileProbabilities=quantileProbabilities,\n",
    "                            quantilesCol=\"quantiles\")\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(testingDf)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableSurvivalRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableSurvivalRegression\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID': 'INT'})\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        survivalRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            x = f\"INSERT INTO {schema}.ScoresTableSurvivalRegression (MOMENT_ID,censor) VALUES (\" + str(moment_id) + \",\" + str(val[1]) + \")\"\n",
    "            splice.execute(x)\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            d = float(survivalRegressionDf.collect()[i][-2])\n",
    "            tab = float(splice.df(f\"SELECT PREDICTION FROM {schema}.ScoresTableSurvivalRegression WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "            l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "            assert round(d,l) == round(tab,l), f'Problem. {round(d,l)} from model, {round(tab,l)} from table'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.7.7, pytest-6.2.1, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /tmp/tmpo04hjmt9\n",
      "plugins: notebook-0.6.1, anyio-3.1.0\n",
      "collected 1 item\n",
      "\n",
      "_ipytesttmp.py F                                                         [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "____________________ test_multilayer_perceptron_classifier _____________________\n",
      "\n",
      "    def test_multilayer_perceptron_classifier():\n",
      "    \n",
      "        print('========== Testing Multi Layer Perceptron ==========')\n",
      "        with mlflow.start_run(run_name='Multi Layer Perceptron'):\n",
      "            model = MultilayerPerceptronClassifier(layers=[10,7,2])\n",
      "            model = Pipeline(stages=[assembler,model])\n",
      "            model = model.fit(testingDf)\n",
      "            mlflow.log_model(model,'model')\n",
      "    \n",
      "            splice.dropTableIfExists(f\"{schema}.ScoresTableMLPC\")\n",
      "    \n",
      "            jid = mlflow.deploy_db(db_schema_name=schema, db_table_name=\"ScoresTableMLPC\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT'}, classes=[\"0\",\"1\"])\n",
      ">           mlflow.watch_job(jid)\n",
      "\n",
      "_ipytesttmp.py:74: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "job_id = 149\n",
      "\n",
      "    @_mlflow_patch('watch_job')\n",
      "    def _watch_job(job_id: int):\n",
      "        \"\"\"\n",
      "        Stream the logs in real time to standard out\n",
      "        of a Job\n",
      "    \n",
      "        :param job_id: the job id to watch (returned after executing an operation)\n",
      "        :raise SpliceMachineException: If the job being watched fails\n",
      "        \"\"\"\n",
      "        previous_lines = []\n",
      "        warn = False # If there were any warnings from the log, we want to notify the user explicitly\n",
      "        while True:\n",
      "            logs_retrieved = __get_logs(job_id)\n",
      "            logs_retrieved.remove('')\n",
      "            log_idx = len(logs_retrieved)\n",
      "            # searching from the end is faster, because unless the logs double in the interval, it will be closer\n",
      "            for log_idx in range(len(logs_retrieved) - 1, -1, -1):\n",
      "                if logs_retrieved[log_idx] in previous_lines:\n",
      "                    break\n",
      "    \n",
      "            idx = log_idx+1 if log_idx else log_idx # First time getting logs, go to 0th index, else log_idx+1\n",
      "            for n in logs_retrieved[idx:]:\n",
      "                if 'WARNING' in n:\n",
      "                    warnings.warn(n)\n",
      "                    warn = True\n",
      "                print(f'\\n{n}',end='')\n",
      "    \n",
      "            previous_lines = copy.deepcopy(logs_retrieved)  # O(1) checking\n",
      "            previous_lines = previous_lines if previous_lines[-1] else previous_lines[:-1] # Remove empty line\n",
      "            if 'TASK_COMPLETED' in previous_lines[-1]: # Finishing Statement\n",
      "                # Check for a failure first, and raise an error if so\n",
      "                for log in reversed(previous_lines):\n",
      "                    if 'ERROR' in log and 'Task Failed' in log:\n",
      "                        raise SpliceMachineException(\n",
      "                            'An error occured in your Job. See the log above for more information'\n",
      ">                       ) from None\n",
      "E                       splicemachine.SpliceMachineException: An error occured in your Job. See the log above for more information\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/splicemachine/mlflow_support/mlflow_support.py:1035: SpliceMachineException\n",
      "----------------------------- Captured stdout call -----------------------------\n",
      "========== Testing Multi Layer Perceptron ==========\n",
      "Uploading file... Done.\n",
      "Deploying model to database...\n",
      "Your Job has been submitted. The returned value of this function is the job id, which you can use to monitor the your task in real-time. Run mlflow.watch_job(<job id>) tostream them to stdout, or mlflow.fetch_logs(<job id>) to read them one time to a list\n",
      "\n",
      "---Job Logs---\n",
      "INFO     2021-06-04 23:11:40.140 - A service worker has found your request\n",
      "INFO     2021-06-04 23:11:40.224 - Checking whether handler DEPLOY_DATABASE is enabled\n",
      "INFO     2021-06-04 23:11:40.315 - Handler is available\n",
      "INFO     2021-06-04 23:11:40.332 - Retrieving Run from MLFlow Tracking Server...\n",
      "INFO     2021-06-04 23:11:40.427 - Retrieved MLFlow Run\n",
      "INFO     2021-06-04 23:11:40.449 - Updating MLFlow Run for the UI\n",
      "INFO     2021-06-04 23:11:40.547 - Reading Model Artifact Stream from Splice Machine\n",
      "INFO     2021-06-04 23:11:40.569 - Extracting Model from DB with Name: model\n",
      "INFO     2021-06-04 23:11:40.603 - Decoding Model Artifact Binary Stream for Deployment\n",
      "INFO     2021-06-04 23:11:40.627 - Decompressing Model Artifact\n",
      "INFO     2021-06-04 23:11:40.650 - Creating raw model representation from MLModel\n",
      "INFO     2021-06-04 23:11:40.671 - Reading MLModel Flavor from Extracted Archive\n",
      "INFO     2021-06-04 23:11:41.330 - Registering Raw Model Representation...\n",
      "INFO     2021-06-04 23:11:41.351 - Done.\n",
      "INFO     2021-06-04 23:11:41.373 - Adding Model Schema and DF...\n",
      "INFO     2021-06-04 23:11:41.437 - Saving Spark Representation to MLeap Format\n",
      "ERROR    2021-06-04 23:11:41.630 - Encountered Unknown Exception while processing...\n",
      "INFO     2021-06-04 23:11:41.649 - Rolling back...\n",
      "INFO     2021-06-04 23:11:41.674 - Cleaning up deployment\n",
      "ERROR    2021-06-04 23:11:41.724 - Task Failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/threading.py\", line 885, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "    │    └ <function Thread._bootstrap_inner at 0x7f02fb68c8c8>\n",
      "    └ <Worker(Thread-17, started 139647095138048)>\n",
      "  File \"/usr/local/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "    │    └ <function Worker.run at 0x7f025cf36048>\n",
      "    └ <Worker(Thread-17, started 139647095138048)>\n",
      "  File \"/usr/local/lib/python3.7/site-packages/workerpool/workers.py\", line 36, in run\n",
      "    job.run()\n",
      "    │   └ <function Runner.run at 0x7f025cf36d90>\n",
      "    └ <main.Runner object at 0x7f021c0222b0>\n",
      "\n",
      "  File \"/opt/bobby/main.py\", line 128, in run\n",
      "    KnownHandlers.get_class(self.handler_name)(self.task_id).handle()\n",
      "    │             │         │    │             │    └ 149\n",
      "    │             │         │    │             └ <main.Runner object at 0x7f021c0222b0>\n",
      "    │             │         │    └ 'DEPLOY_DATABASE'\n",
      "    │             │         └ <main.Runner object at 0x7f021c0222b0>\n",
      "    │             └ <staticmethod object at 0x7f02ce926ac8>\n",
      "    └ <class 'shared.services.handlers.KnownHandlers'>\n",
      "\n",
      "> File \"/opt/bobby/handlers/base_handler.py\", line 85, in handle\n",
      "    self._handle()\n",
      "    │    └ <function BaseDeploymentHandler._handle at 0x7f02ce85c598>\n",
      "    └ <handlers.run_handlers.database_deployment_handler.DatabaseDeploymentHandler object at 0x7f025c19fcf8>\n",
      "\n",
      "  File \"/opt/bobby/handlers/run_handlers/base_deployment_handler.py\", line 161, in _handle\n",
      "    self.exception_handler(exc=e)  # can be overriden by subclasses\n",
      "    │    └ <function DatabaseDeploymentHandler.exception_handler at 0x7f025d2030d0>\n",
      "    └ <handlers.run_handlers.database_deployment_handler.DatabaseDeploymentHandler object at 0x7f025c19fcf8>\n",
      "\n",
      "  File \"/opt/bobby/handlers/run_handlers/database_deployment_handler.py\", line 222, in exception_handler\n",
      "    raise exc\n",
      "          └ Py4JJavaError('An error occurred while calling o3777.serializeToBundle.\\n', JavaObject id=o3823)\n",
      "\n",
      "  File \"/opt/bobby/handlers/run_handlers/base_deployment_handler.py\", line 158, in _handle\n",
      "    self.execute()\n",
      "    │    └ <function DatabaseDeploymentHandler.execute at 0x7f025d203158>\n",
      "    └ <handlers.run_handlers.database_deployment_handler.DatabaseDeploymentHandler object at 0x7f025c19fcf8>\n",
      "\n",
      "  File \"/opt/bobby/handlers/run_handlers/database_deployment_handler.py\", line 242, in execute\n",
      "    execute_step()\n",
      "    └ <bound method DatabaseDeploymentHandler._retrieve_alternate_model_representations of <handlers.run_handlers.database_deployme...\n",
      "\n",
      "  File \"/opt/bobby/handlers/run_handlers/database_deployment_handler.py\", line 172, in _retrieve_alternate_model_representations\n",
      "    self.creator.create_alternate_representations()\n",
      "    │    │       └ <function DatabaseRepresentationCreator.create_alternate_representations at 0x7f02cb8b2400>\n",
      "    │    └ <handlers.run_handlers.db_deploy_utils.db_representation_creator.DatabaseRepresentationCreator object at 0x7f021c0c9908>\n",
      "    └ <handlers.run_handlers.database_deployment_handler.DatabaseDeploymentHandler object at 0x7f025c19fcf8>\n",
      "\n",
      "  File \"/opt/bobby/handlers/run_handlers/db_deploy_utils/db_representation_creator.py\", line 93, in create_alternate_representations\n",
      "    self.representation_generator()\n",
      "    │    └ <bound method DatabaseRepresentationCreator._create_alternate_spark of <handlers.run_handlers.db_deploy_utils.db_representati...\n",
      "    └ <handlers.run_handlers.db_deploy_utils.db_representation_creator.DatabaseRepresentationCreator object at 0x7f021c0c9908>\n",
      "\n",
      "  File \"/opt/bobby/handlers/run_handlers/db_deploy_utils/db_representation_creator.py\", line 183, in _create_alternate_spark\n",
      "    raise e from None\n",
      "\n",
      "  File \"/opt/bobby/handlers/run_handlers/db_deploy_utils/db_representation_creator.py\", line 177, in _create_alternate_spark\n",
      "    library_representation.transform(self.model.get_metadata(Metadata.DATAFRAME_EXAMPLE))\n",
      "    │                      │         │    │     │            │        └ 'dataframe_example'\n",
      "    │                      │         │    │     │            └ <class 'shared.models.model_types.Metadata'>\n",
      "    │                      │         │    │     └ <function Model.get_metadata at 0x7f02cb8b21e0>\n",
      "    │                      │         │    └ <handlers.run_handlers.db_deploy_utils.entities.db_model.Model object at 0x7f021c0228d0>\n",
      "    │                      │         └ <handlers.run_handlers.db_deploy_utils.db_representation_creator.DatabaseRepresentationCreator object at 0x7f021c0c9908>\n",
      "    │                      └ <function Transformer.transform at 0x7f025d3c12f0>\n",
      "    └ PipelineModel_8d1dc310984b\n",
      "\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mleap/pyspark/spark_support.py\", line 25, in serializeToBundle\n",
      "    serializer.serializeToBundle(self, path, dataset=dataset)\n",
      "    │          │                 │     │             └ DataFrame[S1: int, C1: int, S2: int, C2: int, S3: int, C3: int, S4: int, C4: int, S5: int, C5: int, features: vector, rawPred...\n",
      "    │          │                 │     └ 'jar:file:///tmp/tmpcpknc8l0/mleap_model.zip'\n",
      "    │          │                 └ PipelineModel_8d1dc310984b\n",
      "    │          └ <function SimpleSparkSerializer.serializeToBundle at 0x7f021c0a9400>\n",
      "    └ <mleap.pyspark.spark_support.SimpleSparkSerializer object at 0x7f021c0c9358>\n",
      "  File \"/usr/local/lib/python3.7/site-packages/mleap/pyspark/spark_support.py\", line 42, in serializeToBundle\n",
      "    self._java_obj.serializeToBundle(transformer._to_java(), path, dataset._jdf)\n",
      "    │    │                           │           │           │     │       └ JavaObject id=o3776\n",
      "    │    │                           │           │           │     └ DataFrame[S1: int, C1: int, S2: int, C2: int, S3: int, C3: int, S4: int, C4: int, S5: int, C5: int, features: vector, rawPred...\n",
      "    │    │                           │           │           └ 'jar:file:///tmp/tmpcpknc8l0/mleap_model.zip'\n",
      "    │    │                           │           └ <function PipelineModel._to_java at 0x7f025d3da048>\n",
      "    │    │                           └ PipelineModel_8d1dc310984b\n",
      "    │    └ JavaObject id=o3777\n",
      "    └ <mleap.pyspark.spark_support.SimpleSparkSerializer object at 0x7f021c0c9358>\n",
      "  File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "    │       │    │               │    │          │    └ 'serializeToBundle'\n",
      "    │       │    │               │    │          └ <py4j.java_gateway.JavaMember object at 0x7f021c022ba8>\n",
      "    │       │    │               │    └ 'o3777'\n",
      "    │       │    │               └ <py4j.java_gateway.JavaMember object at 0x7f021c022ba8>\n",
      "    │       │    └ <py4j.java_gateway.GatewayClient object at 0x7f025c6b26a0>\n",
      "    │       └ <py4j.java_gateway.JavaMember object at 0x7f021c022ba8>\n",
      "    └ 'xro3823'\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 128, in deco\n",
      "    return f(*a, **kw)\n",
      "           │  │    └ {}\n",
      "           │  └ ('xro3823', <py4j.java_gateway.GatewayClient object at 0x7f025c6b26a0>, 'o3777', 'serializeToBundle')\n",
      "           └ <function get_return_value at 0x7f02cf551f28>\n",
      "  File \"/usr/local/lib/python3.7/site-packages/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "           │               │      └ JavaObject id=o3823\n",
      "           │               └ 'serializeToBundle'\n",
      "           └ 'o3777'\n",
      "\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o3777.serializeToBundle.\n",
      ": java.lang.NoSuchMethodError: org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel.layers()[I\n",
      "\tat org.apache.spark.ml.bundle.ops.classification.MultiLayerPerceptronClassifierOp$$anon$1.store(MultiLayerPerceptronClassifierOp.scala:24)\n",
      "\tat org.apache.spark.ml.bundle.ops.classification.MultiLayerPerceptronClassifierOp$$anon$1.store(MultiLayerPerceptronClassifierOp.scala:14)\n",
      "\tat ml.combust.bundle.serializer.ModelSerializer.$anonfun$write$1(ModelSerializer.scala:87)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:83)\n",
      "\tat ml.combust.bundle.serializer.NodeSerializer.$anonfun$write$1(NodeSerializer.scala:85)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer.$anonfun$writeNode$1(GraphSerializer.scala:34)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer.writeNode(GraphSerializer.scala:30)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer.$anonfun$write$2(GraphSerializer.scala:21)\n",
      "\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n",
      "\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n",
      "\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n",
      "\tat ml.combust.bundle.serializer.GraphSerializer.write(GraphSerializer.scala:21)\n",
      "\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:21)\n",
      "\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:14)\n",
      "\tat ml.combust.bundle.serializer.ModelSerializer.$anonfun$write$1(ModelSerializer.scala:87)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:83)\n",
      "\tat ml.combust.bundle.serializer.NodeSerializer.$anonfun$write$1(NodeSerializer.scala:85)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n",
      "\tat ml.combust.bundle.serializer.BundleSerializer.$anonfun$write$1(BundleSerializer.scala:34)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat ml.combust.bundle.serializer.BundleSerializer.write(BundleSerializer.scala:29)\n",
      "\tat ml.combust.bundle.BundleWriter.save(BundleWriter.scala:34)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.$anonfun$serializeToBundleWithFormat$4(SimpleSparkSerializer.scala:26)\n",
      "\tat resource.AbstractManagedResource.$anonfun$acquireFor$1(AbstractManagedResource.scala:88)\n",
      "\tat scala.util.control.Exception$Catch.$anonfun$either$1(Exception.scala:252)\n",
      "\tat scala.util.control.Exception$Catch.apply(Exception.scala:228)\n",
      "\tat scala.util.control.Exception$Catch.either(Exception.scala:252)\n",
      "\tat resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:88)\n",
      "\tat resource.ManagedResourceOperations.apply(ManagedResourceOperations.scala:26)\n",
      "\tat resource.ManagedResourceOperations.apply$(ManagedResourceOperations.scala:26)\n",
      "\tat resource.AbstractManagedResource.apply(AbstractManagedResource.scala:50)\n",
      "\tat resource.DeferredExtractableManagedResource.$anonfun$tried$1(AbstractManagedResource.scala:33)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat resource.DeferredExtractableManagedResource.tried(AbstractManagedResource.scala:33)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundleWithFormat(SimpleSparkSerializer.scala:25)\n",
      "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundle(SimpleSparkSerializer.scala:17)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "\n",
      "INFO     2021-06-04 23:11:41.847 - TASK_COMPLETED\n",
      "Uploading file... Done.\n",
      "Saved artifact as Multi Layer Perceptron_run_log.ipynb in mlflow\n",
      "Uploading file... Done.\n",
      "Saved artifact as Multi Layer Perceptron_run_log.html in mlflow\n",
      "----------------------------- Captured stderr call -----------------------------\n",
      "[NbConvertApp] Converting notebook /tmp/tmpibcvdlid to html\n",
      "[NbConvertApp] Writing 472701 bytes to /tmp/tmpibcvdli.html\n",
      "=========================== short test summary info ============================\n",
      "FAILED _ipytesttmp.py::test_multilayer_perceptron_classifier - splicemachine....\n",
      "============================== 1 failed in 11.76s ==============================\n"
     ]
    }
   ],
   "source": [
    "%%pytest\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# from splicemachine.spark import PySpliceContext\n",
    "# from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.ml.classification import *\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.ml.feature import *\n",
    "# from pyspark.ml import Pipeline\n",
    "# from splicemachine.mlflow_support import *\n",
    "# import pytest\n",
    "# from splicemachine.mlflow_support import get_user\n",
    "\n",
    "# schema = get_user()\n",
    "\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# splice = PySpliceContext(spark)\n",
    "\n",
    "# handSchema = [StructField(\"S1\", IntegerType(), True),\n",
    "#          StructField(\"C1\", IntegerType(), True),\n",
    "#         StructField(\"S2\", IntegerType(), True),\n",
    "#          StructField(\"C2\", IntegerType(), True),\n",
    "#         StructField(\"S3\", IntegerType(), True),\n",
    "#          StructField(\"C3\", IntegerType(), True),\n",
    "#         StructField(\"S4\", IntegerType(), True),\n",
    "#          StructField(\"C4\", IntegerType(), True),\n",
    "#         StructField(\"S5\", IntegerType(), True),\n",
    "#          StructField(\"C5\", IntegerType(), True),\n",
    "#          StructField(\"label\", IntegerType(), True)]\n",
    "\n",
    "# cardSchema = StructType(handSchema)\n",
    "\n",
    "# assembler = VectorAssembler(inputCols=[\"S1\", \"C1\", \"S2\", \"C2\", \"S3\", \"C3\", \"S4\", \"C4\", \"S5\", \"C5\"], outputCol=\"features\")\n",
    "\n",
    "# testValues = [(1,1,1,13,2,4,2,3,1,12,0),\n",
    "# (3,12,3,2,3,11,4,5,2,5,1),\n",
    "# (1,9,4,6,1,4,3,2,3,9,1),\n",
    "# (1,4,3,13,2,13,2,1,3,6,1),\n",
    "# (3,10,2,7,1,2,2,11,4,9,0),\n",
    "# (1,3,4,5,3,4,1,12,4,6,0),\n",
    "# (2,6,4,11,2,3,4,9,1,7,0),\n",
    "# (3,2,4,9,3,7,4,3,4,5,0),\n",
    "# (4,4,3,13,1,8,3,9,3,10,0),\n",
    "# (1,9,3,8,4,4,1,7,3,5,0),\n",
    "# (4,7,3,12,1,13,1,9,2,6,0),\n",
    "# (2,12,1,3,2,11,2,7,4,8,0),\n",
    "# (4,2,2,9,2,7,1,5,3,11,0),\n",
    "# (1,13,2,6,1,6,2,11,3,5,1),\n",
    "# (3,8,2,7,1,9,3,6,2,3,0),\n",
    "# (2,10,1,11,1,9,3,1,1,13,0),\n",
    "# (4,2,4,12,2,12,2,7,3,10,1),\n",
    "# (4,5,2,2,4,9,1,5,4,1,1),\n",
    "# (2,3,3,9,2,1,2,6,4,10,0),\n",
    "# (1,7,2,11,4,1,2,9,3,13,0)]\n",
    "\n",
    "# testingDf = spark.createDataFrame(testValues, StructType(cardSchema))\n",
    "\n",
    "# testingDf.show()\n",
    "\n",
    "# mlflow.register_splice_context(splice)\n",
    "\n",
    "# def test_multilayer_perceptron_classifier():\n",
    "\n",
    "#     print('========== Testing Multi Layer Perceptron ==========')\n",
    "#     with mlflow.start_run(run_name='Multi Layer Perceptron'):\n",
    "#         model = MultilayerPerceptronClassifier(layers=[10,7,2])\n",
    "#         model = Pipeline(stages=[assembler,model])\n",
    "#         model = model.fit(testingDf)\n",
    "#         mlflow.log_model(model,'model')\n",
    "\n",
    "#         splice.dropTableIfExists(f\"{schema}.ScoresTableMLPC\")\n",
    "\n",
    "#         jid = mlflow.deploy_db(db_schema_name=schema, db_table_name=\"ScoresTableMLPC\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT'}, classes=[\"0\",\"1\"])\n",
    "#         mlflow.watch_job(jid)\n",
    "\n",
    "#         MLPCDf = model.transform(testingDf)   \n",
    "\n",
    "#         for moment_id,val in enumerate(testValues):\n",
    "#             splice.execute(f\"INSERT INTO {schema}.ScoresTableMLPC (MOMENT_ID,S1,C1,S2,C2,S3,C3,S4,C4,S5,C5) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \",\" + str(val[2]) + \",\" + str(val[3]) + \",\" + str(val[4]) + \",\" + str(val[5]) + \",\" + str(val[6]) + \",\" + str(val[7]) + \",\" + str(val[8]) + \",\" + str(val[9]) + \")\")\n",
    "\n",
    "\n",
    "#         for i,row in enumerate(MLPCDf.collect()):\n",
    "#             dfp = [row[-1]] + [idx for idx in row[-2]]\n",
    "#             table_pred = list(splice.df(f'SELECT cast(PREDICTION as int) PREDICTION,\"0\",\"1\" FROM {schema}.ScoresTableMLPC WHERE MOMENT_ID = {i}').collect()[0])        \n",
    "\n",
    "#             for tab, d in zip(table_pred, dfp):\n",
    "#                 l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "#                 assert round(d,l) == round(tab,l), f'Problem. {d} from model, {tab} from table'\n",
    "            \n",
    "        \n",
    "# def test_one_vs_rest(): MLeap doesn't support ovr at the moment (0.15.0)\n",
    "\n",
    "#     print('========== Testing OVR ==========')\n",
    "#     with mlflow.start_run(run_name='Multi Layer Perceptron'):\n",
    "#         model = MultilayerPerceptronClassifier(layers=[10,7,2])\n",
    "#         model = OneVsRest(classifier=model)\n",
    "#         model = Pipeline(stages=[assembler,model])\n",
    "#         model = model.fit(testingDf)\n",
    "#         mlflow.log_model(model,'model')\n",
    "    \n",
    "#         splice.dropTableIfExists(f\"{schema}.ScoresTableOVR\")\n",
    "\n",
    "#         jid = mlflow.deploy_db(db_schema_name=schema, db_table_name=\"ScoresTableOVR\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT'})\n",
    "#         mlflow.watch_job(jid)\n",
    "        \n",
    "#         OVRDf = model.transform(testingDf)   \n",
    "\n",
    "#         for moment_id,val in enumerate(testValues):\n",
    "#             splice.execute(f\"INSERT INTO {schema}.ScoresTableOVR (MOMENT_ID,S1,C1,S2,C2,S3,C3,S4,C4,S5,C5) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \",\" + str(val[2]) + \",\" + str(val[3]) + \",\" + str(val[4]) + \",\" + str(val[5]) + \",\" + str(val[6]) + \",\" + str(val[7]) + \",\" + str(val[8]) + \",\" + str(val[9]) + \")\")\n",
    "\n",
    "#         for i in range (len(testValues)):\n",
    "#             assert float(OVRDf.collect()[i][-1]) == float(splice.df(f\"SELECT PREDICTION FROM {schema}.ScoresTableOVR WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
