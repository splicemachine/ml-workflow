{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pytest\n",
    "!pip install pytest-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/akaihola/ipython_pytest/master/ipython_pytest.py -O ipython_pytest.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ipython_pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark import PySpliceContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from splicemachine.mlflow_support import *\n",
    "import pytest\n",
    "from splicemachine.mlflow_support import get_user\n",
    "\n",
    "schema = get_user()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "splice = PySpliceContext(spark)\n",
    "\n",
    "scoreSchema = [StructField(\"Score1\", DoubleType(), True),\n",
    "         StructField(\"Score2\", DoubleType(), True),\n",
    "         StructField(\"Result\", IntegerType(), True)]\n",
    "\n",
    "examSchema = StructType(scoreSchema)\n",
    "\n",
    "df = spark.read.schema(examSchema).load(\"s3a://zach-splice-bucket/src_main_resources_scores.csv\", format=\"csv\").withColumnRenamed('Result','label')\n",
    "\n",
    "df.show()\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"Score1\", \"Score2\"], outputCol=\"features\")\n",
    "\n",
    "testingSchema = [StructField(\"Score1\", DoubleType(), True),\n",
    "         StructField(\"Score2\", DoubleType(), True)]\n",
    "testValues = [(40.0,40.0),(20.0,80.0),(20.0,30.0),(90.0,80.0),(10.0,20.0),\n",
    "               (10.0,50.0),(50.0,30.0),(40.0,75.0),(90.0,30.0),(100.0,100.0)]\n",
    "testingDf = spark.createDataFrame(testValues, StructType(testingSchema))\n",
    "\n",
    "mlflow.register_splice_context(splice)\n",
    "\n",
    "\n",
    "def test_logistic_regression():\n",
    "    \n",
    "    print('========== Testing Logistic Regression ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='Logistic Regression'):\n",
    "    \n",
    "        model = LogisticRegression()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableLogisticRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableLogisticRegression\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        LogisticRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableLogisticRegression (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "\n",
    "        splice.execute(f\"select * from {schema}.ScoresTableLogisticRegression\")\n",
    "\n",
    "        for i,row in enumerate(LogisticRegressionDf.collect()):\n",
    "            dfp = [row[5],row[4][0],row[4][1]]\n",
    "            table_pred = list(splice.df(f'SELECT case when prediction=\\'C0\\' then 0 else 1 end PREDICTION,\"C0\",\"C1\" FROM SCORESTABLELOGISTICREGRESSION WHERE MOMENT_ID = {i}').collect()[0])\n",
    "            assert dfp == table_pred, f'Problem. {dfp} from model, {table_pred} from table'\n",
    "\n",
    "def test_decision_tree():\n",
    "    \n",
    "    print('========== Testing Decision Tree Classification ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='Decision Tree Classification'):\n",
    "        \n",
    "        model = DecisionTreeClassifier()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableDecisionTree\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableDecisionTree\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        DecisionTreeDf = model.transform(testingDf)\n",
    "\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableDecisionTree (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "\n",
    "        splice.execute(f\"select * from {schema}.ScoresTableDecisionTree\")\n",
    "\n",
    "        for i,row in enumerate(DecisionTreeDf.collect()):\n",
    "            dfp = [row[5],row[4][0],row[4][1]]\n",
    "            table_pred = list(splice.df(f'SELECT case when prediction=\\'C0\\' then 0 else 1 end PREDICTION,\"C0\",\"C1\" FROM {schema}.ScoresTableDecisionTree WHERE MOMENT_ID = {i}').collect()[0])\n",
    "            \n",
    "            for tab, d in zip(table_pred, dfp):\n",
    "                l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "                assert round(d,l) == round(tab,l), f'Problem. {d} from model, {tab} from table'\n",
    "        \n",
    "def test_random_forest():\n",
    "    \n",
    "    print('========== Testing Random Forest ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='Random Forest'):\n",
    "    \n",
    "        model = DecisionTreeClassifier()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableRandomForest\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableRandomForest\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)    \n",
    "            \n",
    "        RandomForestDf = model.transform(testingDf)\n",
    "\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableRandomForest (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        splice.execute(f\"select * from {schema}.ScoresTableRandomForest\")\n",
    "\n",
    "        for i,row in enumerate(RandomForestDf.collect()):\n",
    "            dfp = [row[5],row[4][0],row[4][1]]\n",
    "            table_pred = list(splice.df(f'SELECT case when prediction=\\'C0\\' then 0 else 1 end PREDICTION,\"C0\",\"C1\" FROM {schema}.ScoresTableRandomForest WHERE MOMENT_ID = {i}').collect()[0])\n",
    "            for tab, d in zip(table_pred, dfp):\n",
    "                l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "                assert round(d,l) == round(tab,l), f'Problem. {d} from model, {tab} from table'\n",
    "\n",
    "def test_gbt():  \n",
    "    \n",
    "    print('========== Testing GBT ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='GBT'):\n",
    "\n",
    "        model = GBTClassifier()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableGBT\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableGBT\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        GBTDf = model.transform(testingDf)\n",
    "\n",
    "        \n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableGBT (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        for i,row in enumerate(GBTDf.collect()):\n",
    "            dfp = [row[5],row[4][0],row[4][1]]\n",
    "            table_pred = list(splice.df(f'SELECT case when prediction=\\'C0\\' then 0 else 1 end PREDICTION,\"C0\",\"C1\" FROM {schema}.ScoresTableGBT WHERE MOMENT_ID = {i}').collect()[0])\n",
    "            for tab, d in zip(table_pred, dfp):\n",
    "                l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "                assert round(d,l) == round(tab,l), f'Problem. {d} from model, {tab} from table'\n",
    "\n",
    "def test_lsvc():\n",
    "    \n",
    "    print('========== Testing LSVC ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='Linear SVC'):\n",
    "        model = LinearSVC()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableLSVC\")\n",
    "\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableLSVC\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        LSVCDf = model.transform(testingDf)\n",
    "\n",
    "        LSVCDf.show()\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableLSVC (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        LSVCDf.show()\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            assert int(LSVCDf.collect()[i][-1]) == int(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLELSVC WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "\n",
    "def test_naive_bayes():\n",
    "    \n",
    "    print('========== Testing Naive Bayes ==========')\n",
    "    \n",
    "    with mlflow.start_run(run_name='Naive Bayes'):\n",
    "        model = NaiveBayes()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "    \n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableNaiveBayes\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableNaiveBayes\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'}, classes=[\"C0\",\"C1\"])\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        NaiveBayesDf = model.transform(testingDf)\n",
    "\n",
    "        NaiveBayesDf.show()\n",
    "\n",
    "        \n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableNaiveBayes (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        NaiveBayesDf.show()\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            assert int(NaiveBayesDf.collect()[i][5]) == int(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLENAIVEBAYES WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "\n",
    "def test_decision_tree_regression():\n",
    "    \n",
    "    print('========== Testing Decision Tree Regression ==========')\n",
    "    with mlflow.start_run(run_name='Decision Tree Regression'):\n",
    "        model = DecisionTreeRegressor()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableDecisionTreeRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableDecisionTreeRegression\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'})\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        DecisionTreeRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        DecisionTreeRegressionDf.show()\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableDecisionTreeRegression (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        DecisionTreeRegressionDf.show()\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            assert float(DecisionTreeRegressionDf.collect()[i][3]) == float(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLEDECISIONTREEREGRESSION WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "\n",
    "def test_random_forest_regression():\n",
    "    \n",
    "    print('========== Testing Random Forest Regression ==========')\n",
    "    with mlflow.start_run(run_name='Random Forest Regression'):\n",
    "        model = RandomForestRegressor()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "    \n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableRandomForestRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableRandomForestRegression\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'})\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        RandomForestRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        RandomForestRegressionDf.show()\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableRandomForestRegression (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        RandomForestRegressionDf.show()\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            assert float(RandomForestRegressionDf.collect()[i][3]) == float(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLERANDOMFORESTREGRESSION WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "        \n",
    "def test_gradient_boosted_tree_regression():\n",
    "    \n",
    "    print('========== Testing GBT Regression ==========')\n",
    "    with mlflow.start_run(run_name='GBT Regression'):\n",
    "        model = GBTRegressor()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(df)\n",
    "        mlflow.log_model(model,'model')\n",
    "    \n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableGBTRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableGBTRegression\", run_id=mlflow.current_run_id(), df=df.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT','Score1':'DOUBLE','Score2':'DOUBLE'})\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        GBTRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        GBTRegressionDf.show()\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableGBTRegression (MOMENT_ID,Score1,Score2) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \")\")\n",
    "            \n",
    "\n",
    "        GBTRegressionDf.show()\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            assert float(GBTRegressionDf.collect()[i][3]) == float(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLEGBTREGRESSION WHERE MOMENT_ID = {i}\").collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark import PySpliceContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from splicemachine.mlflow_support import *\n",
    "import pytest\n",
    "import pyspark.sql.functions as F\n",
    "from splicemachine.mlflow_support import get_user\n",
    "\n",
    "schema = get_user()\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "splice = PySpliceContext(spark)\n",
    "\n",
    "testValues = [(391,314,102,2,2.0,2.5,8.24,0,0.64),\n",
    "(392,318,106,3,2.0,3.0,8.65,0,0.71),\n",
    "(393,326,112,4,4.0,3.5,9.12,1,0.84),\n",
    "(394,317,104,2,3.0,3.0,8.76,0,0.77),\n",
    "(395,329,111,4,4.5,4.0,9.23,1,0.89),\n",
    "(396,324,110,3,3.5,3.5,9.04,1,0.82),\n",
    "(397,325,107,3,3.0,3.5,9.11,1,0.84),\n",
    "(398,330,116,4,5.0,4.5,9.45,1,0.91),\n",
    "(399,312,103,3,3.5,4.0,8.78,0,0.67),\n",
    "(400,333,117,4,5.0,4.0,9.66,1,0.95)]\n",
    "\n",
    "admitSchema = [StructField(\"Serial Number\", IntegerType(), True),\n",
    "        StructField(\"GRE\", IntegerType(), True),\n",
    "         StructField(\"TOEFL\", IntegerType(), True),\n",
    "        StructField(\"Rating\", IntegerType(), True),\n",
    "         StructField(\"SOP\", DoubleType(), True),\n",
    "        StructField(\"LOR\", DoubleType(), True),\n",
    "         StructField(\"CGPA\", DoubleType(), True),\n",
    "        StructField(\"Research\", IntegerType(), True),\n",
    "         StructField(\"label\", DoubleType(), True)]\n",
    "\n",
    "collegeSchema = StructType(admitSchema)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"GRE\", \"TOEFL\", \"Rating\", \"SOP\", \"LOR\", \"CGPA\", \"Research\"], outputCol=\"features\")\n",
    "\n",
    "testingDf = spark.createDataFrame(testValues, StructType(collegeSchema))\n",
    "\n",
    "mlflow.register_splice_context(splice)\n",
    "\n",
    "def test_linear_regression():\n",
    "    \n",
    "    print('========== Testing Linear Regression ==========')\n",
    "    with mlflow.start_run(run_name='Linear Regression'):\n",
    "        model = LinearRegression()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(testingDf)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableLinearRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableLinearRegression\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\").drop(\"Serial Number\"), create_model_table=True, primary_key={'MOMENT_ID': 'INT'})\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        LinearRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableLinearRegression (MOMENT_ID,GRE,TOEFL,Rating,SOP,LOR,CGPA,Research) VALUES (\" + str(moment_id) + \",\" + str(val[1]) + \",\" + str(val[2]) + \",\" + str(val[3]) + \",\" + str(val[4]) + \",\" + str(val[5]) + \",\" + str(val[6]) + \",\" + str(val[7]) + \")\")\n",
    "            \n",
    "        for i in range (len(testValues)):\n",
    "            d = float(LinearRegressionDf.collect()[i][10])\n",
    "            tab = float(splice.df(f\"SELECT PREDICTION FROM {schema}.SCORESTABLELINEARREGRESSION WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "            l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "            assert round(d,l) == round(tab,l), f'Problem. {round(d,l)} from model, {round(tab,l)} from table'\n",
    "        \n",
    "def test_generalized_linear_regression():\n",
    "   \n",
    "    print('========== Testing Generalized Linear Regression ==========')\n",
    "    with mlflow.start_run(run_name='Generalized Linear Regression'):\n",
    "        model = GeneralizedLinearRegression()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(testingDf)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableGeneralizedLinearRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableGeneralizedLinearRegression\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\").drop(\"Serial Number\"), create_model_table=True, primary_key={'MOMENT_ID': 'INT'})\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        GeneralizedLinearRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableGeneralizedLinearRegression (MOMENT_ID,GRE,TOEFL,Rating,SOP,LOR,CGPA,Research) VALUES (\" + str(moment_id) + \",\" + str(val[1]) + \",\" + str(val[2]) + \",\" + str(val[3]) + \",\" + str(val[4]) + \",\" + str(val[5]) + \",\" + str(val[6]) + \",\" + str(val[7]) + \")\")\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            d = float(GeneralizedLinearRegressionDf.collect()[i][10])\n",
    "            tab = float(splice.df(f\"SELECT PREDICTION FROM {schema}.ScoresTableGeneralizedLinearRegression WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "            l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "            assert round(d,l) == round(tab,l), f'Problem. {round(d,l)} from model, {round(tab,l)} from table'\n",
    "\n",
    "\n",
    "def test_isotonic_regression():\n",
    "   \n",
    "    print('========== Testing Isotonic Regression ==========')\n",
    "    with mlflow.start_run(run_name='Isotonic Regression'):\n",
    "        model = IsotonicRegression()\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(testingDf)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableIsotonicRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableIsotonicRegression\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\").drop(\"Serial Number\"), create_model_table=True, primary_key={'MOMENT_ID': 'INT'})\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        isotonicRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableIsotonicRegression (MOMENT_ID,GRE,TOEFL,Rating,SOP,LOR,CGPA,Research) VALUES (\" + str(moment_id) + \",\" + str(val[1]) + \",\" + str(val[2]) + \",\" + str(val[3]) + \",\" + str(val[4]) + \",\" + str(val[5]) + \",\" + str(val[6]) + \",\" + str(val[7]) + \")\")\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            d = float(isotonicRegressionDf.collect()[i][10])\n",
    "            tab = float(splice.df(f\"SELECT PREDICTION FROM {schema}.ScoresTableIsotonicRegression WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "            l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "            assert round(d,l) == round(tab,l), f'Problem. {round(d,l)} from model, {round(tab,l)} from table'\n",
    "            \n",
    "def test_survival_regression():\n",
    "   \n",
    "    print('========== Testing Survival Regression ==========')\n",
    "    \n",
    "    testValues = [\n",
    "        (1.218, 1.0),\n",
    "        (2.949, 0.0),\n",
    "        (3.627, 0.0),\n",
    "        (0.273, 1.0),\n",
    "        (4.199, 0.0)\n",
    "    ]\n",
    "    \n",
    "    testingDf = spark.createDataFrame(testValues, [\"label\", \"censor\"])\n",
    "    \n",
    "    with mlflow.start_run(run_name='Survival Regression'):\n",
    "        \n",
    "        assembler = VectorAssembler(inputCols=[\"censor\"], outputCol=\"features\")\n",
    "        quantileProbabilities = [0.3, 0.6]\n",
    "        model = AFTSurvivalRegression(quantileProbabilities=quantileProbabilities,\n",
    "                            quantilesCol=\"quantiles\")\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(testingDf)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableSurvivalRegression\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=f\"{schema}\", db_table_name=\"ScoresTableSurvivalRegression\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID': 'INT'})\n",
    "        mlflow.watch_job(jid)\n",
    "        \n",
    "        survivalRegressionDf = model.transform(testingDf)\n",
    "\n",
    "        for moment_id, val in enumerate(testValues):\n",
    "            x = f\"INSERT INTO {schema}.ScoresTableSurvivalRegression (MOMENT_ID,censor) VALUES (\" + str(moment_id) + \",\" + str(val[1]) + \")\"\n",
    "            splice.execute(x)\n",
    "\n",
    "        for i in range (len(testValues)):\n",
    "            d = float(survivalRegressionDf.collect()[i][-2])\n",
    "            tab = float(splice.df(f\"SELECT PREDICTION FROM {schema}.ScoresTableSurvivalRegression WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "            l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "            assert round(d,l) == round(tab,l), f'Problem. {round(d,l)} from model, {round(tab,l)} from table'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark import PySpliceContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml import Pipeline\n",
    "from splicemachine.mlflow_support import *\n",
    "import pytest\n",
    "from splicemachine.mlflow_support import get_user\n",
    "\n",
    "schema = get_user()\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "splice = PySpliceContext(spark)\n",
    "\n",
    "handSchema = [StructField(\"S1\", IntegerType(), True),\n",
    "         StructField(\"C1\", IntegerType(), True),\n",
    "        StructField(\"S2\", IntegerType(), True),\n",
    "         StructField(\"C2\", IntegerType(), True),\n",
    "        StructField(\"S3\", IntegerType(), True),\n",
    "         StructField(\"C3\", IntegerType(), True),\n",
    "        StructField(\"S4\", IntegerType(), True),\n",
    "         StructField(\"C4\", IntegerType(), True),\n",
    "        StructField(\"S5\", IntegerType(), True),\n",
    "         StructField(\"C5\", IntegerType(), True),\n",
    "         StructField(\"label\", IntegerType(), True)]\n",
    "\n",
    "cardSchema = StructType(handSchema)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"S1\", \"C1\", \"S2\", \"C2\", \"S3\", \"C3\", \"S4\", \"C4\", \"S5\", \"C5\"], outputCol=\"features\")\n",
    "\n",
    "testValues = [(1,1,1,13,2,4,2,3,1,12,0),\n",
    "(3,12,3,2,3,11,4,5,2,5,1),\n",
    "(1,9,4,6,1,4,3,2,3,9,1),\n",
    "(1,4,3,13,2,13,2,1,3,6,1),\n",
    "(3,10,2,7,1,2,2,11,4,9,0),\n",
    "(1,3,4,5,3,4,1,12,4,6,0),\n",
    "(2,6,4,11,2,3,4,9,1,7,0),\n",
    "(3,2,4,9,3,7,4,3,4,5,0),\n",
    "(4,4,3,13,1,8,3,9,3,10,0),\n",
    "(1,9,3,8,4,4,1,7,3,5,0),\n",
    "(4,7,3,12,1,13,1,9,2,6,0),\n",
    "(2,12,1,3,2,11,2,7,4,8,0),\n",
    "(4,2,2,9,2,7,1,5,3,11,0),\n",
    "(1,13,2,6,1,6,2,11,3,5,1),\n",
    "(3,8,2,7,1,9,3,6,2,3,0),\n",
    "(2,10,1,11,1,9,3,1,1,13,0),\n",
    "(4,2,4,12,2,12,2,7,3,10,1),\n",
    "(4,5,2,2,4,9,1,5,4,1,1),\n",
    "(2,3,3,9,2,1,2,6,4,10,0),\n",
    "(1,7,2,11,4,1,2,9,3,13,0)]\n",
    "\n",
    "testingDf = spark.createDataFrame(testValues, StructType(cardSchema))\n",
    "\n",
    "testingDf.show()\n",
    "\n",
    "mlflow.register_splice_context(splice)\n",
    "\n",
    "def test_multilayer_perceptron_classifier():\n",
    "\n",
    "    print('========== Testing Multi Layer Perceptron ==========')\n",
    "    with mlflow.start_run(run_name='Multi Layer Perceptron'):\n",
    "        model = MultilayerPerceptronClassifier(layers=[10,7,2])\n",
    "        model = Pipeline(stages=[assembler,model])\n",
    "        model = model.fit(testingDf)\n",
    "        mlflow.log_model(model,'model')\n",
    "\n",
    "        splice.dropTableIfExists(f\"{schema}.ScoresTableMLPC\")\n",
    "\n",
    "        jid = mlflow.deploy_db(db_schema_name=schema, db_table_name=\"ScoresTableMLPC\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT'}, classes=[\"0\",\"1\"])\n",
    "        mlflow.watch_job(jid)\n",
    "\n",
    "        MLPCDf = model.transform(testingDf)   \n",
    "\n",
    "        for moment_id,val in enumerate(testValues):\n",
    "            splice.execute(f\"INSERT INTO {schema}.ScoresTableMLPC (MOMENT_ID,S1,C1,S2,C2,S3,C3,S4,C4,S5,C5) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \",\" + str(val[2]) + \",\" + str(val[3]) + \",\" + str(val[4]) + \",\" + str(val[5]) + \",\" + str(val[6]) + \",\" + str(val[7]) + \",\" + str(val[8]) + \",\" + str(val[9]) + \")\")\n",
    "\n",
    "\n",
    "        for i,row in enumerate(MLPCDf.collect()):\n",
    "            dfp = [row[-1]] + [idx for idx in row[-2]]\n",
    "            table_pred = list(splice.df(f'SELECT cast(PREDICTION as int) PREDICTION,\"0\",\"1\" FROM {schema}.ScoresTableMLPC WHERE MOMENT_ID = {i}').collect()[0])        \n",
    "\n",
    "            for tab, d in zip(table_pred, dfp):\n",
    "                l = min(len(str(tab)), len(str(d)), 15) - 2\n",
    "                assert round(d,l) == round(tab,l), f'Problem. {d} from model, {tab} from table'\n",
    "            \n",
    "        \n",
    "# def test_one_vs_rest(): MLeap doesn't support ovr at the moment (0.15.0)\n",
    "\n",
    "#     print('========== Testing OVR ==========')\n",
    "#     with mlflow.start_run(run_name='Multi Layer Perceptron'):\n",
    "#         model = MultilayerPerceptronClassifier(layers=[10,7,2])\n",
    "#         model = OneVsRest(classifier=model)\n",
    "#         model = Pipeline(stages=[assembler,model])\n",
    "#         model = model.fit(testingDf)\n",
    "#         mlflow.log_model(model,'model')\n",
    "    \n",
    "#         splice.dropTableIfExists(f\"{schema}.ScoresTableOVR\")\n",
    "\n",
    "#         jid = mlflow.deploy_db(db_schema_name=schema, db_table_name=\"ScoresTableOVR\", run_id=mlflow.current_run_id(), df=testingDf.drop(\"label\"), create_model_table=True, primary_key={'MOMENT_ID':'INT'})\n",
    "#         mlflow.watch_job(jid)\n",
    "        \n",
    "#         OVRDf = model.transform(testingDf)   \n",
    "\n",
    "#         for moment_id,val in enumerate(testValues):\n",
    "#             splice.execute(f\"INSERT INTO {schema}.ScoresTableOVR (MOMENT_ID,S1,C1,S2,C2,S3,C3,S4,C4,S5,C5) VALUES (\" + str(moment_id) + \",\" + str(val[0]) + \",\" + str(val[1]) + \",\" + str(val[2]) + \",\" + str(val[3]) + \",\" + str(val[4]) + \",\" + str(val[5]) + \",\" + str(val[6]) + \",\" + str(val[7]) + \",\" + str(val[8]) + \",\" + str(val[9]) + \")\")\n",
    "\n",
    "#         for i in range (len(testValues)):\n",
    "#             assert float(OVRDf.collect()[i][-1]) == float(splice.df(f\"SELECT PREDICTION FROM {schema}.ScoresTableOVR WHERE MOMENT_ID = {i}\").collect()[0][0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
